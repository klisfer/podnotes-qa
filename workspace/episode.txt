The following is a conversation with Max Tegmark, his third time in the podcast.
In fact, his first appearance was episode number one of this very podcast.
He is a physicist and artificial intelligence researcher at MIT co-founder
of Future Life Institute and author of life 3.0 being human in the age of
artificial intelligence.
Most recently, he's a key figure in spearheading the open letter calling for
six month pause on giant AI experiments like training GPT-4.
The letter reads, we're calling for pause on training of models
larger than GPT-4 for six months.
This does not imply a pause or ban on all AI research and development or the
use of systems that have already been placed in the market.
Our call is specific and addresses a very small pool of actors who
possesses this capability.
The letter has been signed by over 50,000 individuals, including 1800 CEOs
and over 1500 professors.
Signatories include Joshua Benjio, Stuart Russell, Elon Musk, Steve Wozniak, you
all know a Harari, Andrew Yang and many others.
This is a defining moment in the history of human civilization, where the balance
of power between human and AI begins to shift.
And Max's mind and his voice is one of the most valuable and powerful in a
time like this.
His support, his wisdom, his friendship has been a gift of forever, deeply
grateful for.
And now a quick few second mention of each sponsor.
Check them out in the description.
It's the best way to support this podcast.
We got notion for project and team collaboration inside tracker for
biological data and indeed for hiring.
Choose wise than my friends.
Also, speaking of hiring, if you want to work with our amazing team, we're
always hiring, whether it's through Indeed or otherwise, could
elects-freatment.com/hiring.
And now onto the full ad reads.
As always, no ads in the middle.
I try to make this interesting, but if you must skip them, please
still check out our sponsors.
I enjoy this stuff.
Maybe you will too.
This show is brought to you by notion.
Spoken endlessly about how amazing notion is, how everybody, all the
cool kids are recommending it for just basic note taking.
But there's so, so much more.
It's the collaborative aspect of it, the project management aspect of it,
the wikis, the documentary, all of that, all in a simple, powerful,
beautifully defined solution.
What can I say?
On top of this, there's the notion AI tool.
This is the best integration of large language models into a
productivity note-taking tool.
There are so many amazing features.
I mean, it's just endless.
Go to the website.
You can generate entire presentations and reports based on a to-do list.
You can summarize stuff, you can short stuff, you can generate tables
based on the description.
You can write a summary, you can expand the text, you can change the
style of the text, you can fix spelling and grammar, you can translate, you can.
You simple a language, more complicated language, change the tone of the voice,
make it shorter, longer.
Like I said, everything is just so easy to play around with and all of it, no
matter what you're doing will challenge you to think how you write.
It will challenge you to expand the style of your writing.
It will save you a lot of time, of course, but I just think it makes you a better
thinker and productive being in this world.
And I think that's such a great integration of AI into the productivity
workflow.
To me, it's not enough for a large language model to be effective at answering
questions and having good dialogue, you have to really integrate it into the
workflow and notion better than anybody else I've seen has done that.
So if that's interesting to you, notion AI helps you work faster, write better
and think bigger during tasks that normally take you hours in just minutes.
Try Notion AI for free when you go to notion.com/lex.
That's all lowercase notion.com/lex to try the power of notion AI today.
This shows also brought to you by InsideTracker, a service I used to track
biological data.
It's really good to do that kind of thing regularly, to look at all the
different markets in your body and to understand what could be made better
through lifestyle and through diet changes.
It's kind of obvious that decisions about your life should be made based on
the data that comes from your own body, not some kind of population study, although
those are good, not some spiritual guru, although those are good, not some
novel, whether it's Harry Potter or Dostoevsky, although those are sometimes good.
Not your relative who says I heard a guy say that a guy does this thing that
is very bro sound, see sounding, although sometimes it turns out to be pretty
effective. Overall, the best decisions about your life should be based on the
things that come from your own body.
InsideTracker uses algorithms to analyze your blood data, DNA data, data, fitness
track, all that kind of stuff to give you recommendations.
You should be doing it, you should be doing it regularly.
So it's not just a one time thing, but regularly over time, you see what changes
led to improvements in the various markers that come from your body.
Because special savings for a limited time when you go to inside tracker.com/lax.
This show is also brought to you by Indeed, a hiring website.
I think the most important thing in life, not to quote Coron the Barbarian
because they'll be very inappropriate to quote at this moment.
And it's not actually accurate at all.
As a reflection of what's important in life, it only has comedic value.
What I really want to say about what's important in life is the people you
surround yourself with.
And we've spent so much of our time in the workplace seeking solutions to very
difficult problems together, passionately pursuing ambitious goals, sometimes
impossible goals, that is the source of meaning, a sort of a happiness for people.
And I think part of that happiness comes from the collaboration with other
human beings, the sort of professional depth of connection that you have with
other human beings of being together through the grind and surviving and
accomplishing the goal or failing in a big epic way, knowing that you have tried
together.
And so doing that with the right team, I think is one of the most important
things in life.
So you should surround yourself with the right team.
If you're looking to join a team, you should be very selective about that.
Or if you're looking to hire a team, you should be very selective about that and
use the best tools of the job.
I've used Indeed many, many times throughout my life for the teams I've led.
Don't overspend on hiring.
Visit Indeed.com/LEX to start hiring now.
That's Indeed.com/LEX terms and conditions apply.
This is the Lex Friedman podcast.
To support it, please check out our sponsors in the description.
And now, dear friends, here's Max.
Tagmark.
[Music]
You were the first ever guest on this podcast, episode number one.
So first of all, Max, I just have to say thank you for giving me a chance.
Thank you for starting this journey.
It's been an incredible journey.
Just thank you for sitting down with me and just acting like I'm somebody who
matters that I'm somebody who's interested in talking to.
And thank you for doing it.
I meant a lot.
Thanks to you for putting your heart and soul into this.
I know when you delve into controversial topics, it's inevitable to get hit by what
Hamlet talks about the slings and arrows and stuff.
And I really, in my sense, it's in an era where YouTube videos are too long and
now it has to be like a 20 minute TikTok, 20 second TikTok clip.
It's just so refreshing to see you going exactly against all of the advice and doing
these really long form things.
And then people appreciate it.
You know, reality is nuanced.
And thanks for sharing it that way.
So let me ask you again, the first question I've ever asked on this podcast, episode
number one, talking to you, do you think there's intelligent life out there in the
universe?
Let's revisit that question.
Do you have any updates?
What's your view when you look out to the stars?
So when we look at the stars, if you define our universe the way most astrophysicists
do, not as all of space, but the spherical region of space that we can see with our
telescopes from which light has the time to reach us since our big bang.
I'm in the minority.
I'm.
And I want to estimate that we are the only life in this spherical volume that has
invented internet radios, gotten our level of tech.
And if that's true, then it puts a lot of responsibility on us to not mess this one up
because if it's true, it means that life is quite rare.
And we are stewards of this one spark of advanced consciousness, which if we
nurture it and help it grow, it eventually life can spread from here out into much of
our universe and we can have this just amazing future, whereas if we instead are
reckless with the technology we build and just snuff it out to the stupidity or in
fighting, then maybe the rest of cosmic history in our universe was just going to
be a play for empty benches.
But I do think that we are actually very likely to get visited by aliens, alien
intelligence quite soon.
But I think we are going to be building that alien intelligence.
So we're going to give birth to an intelligent alien civilization.
Unlike anything, like human, the evolution here on earth was able to create in
terms of the path, the biological path it took.
Yeah, and it's going to be much more alien than a cat or even the most exotic
animal on the planet right now, because it will not have been created through
the usual Darwinian competition where it necessarily cares about self-preservation,
is afraid of death, any of those things.
The space of alien minds that you can build is just so much faster than what
evolution will give you. And with that also comes a great responsibility for us
to make sure that the kind of minds we create are those kind of minds that
it's good to create minds that will share our values and be good for humanity and
life and also create minds that don't suffer.
Do you try to visualize the full space of alien minds that AI could be?
You try to consider all the different kinds of intelligences.
So we're generalizing what humans are able to do to the full spectrum of what
intelligent creatures entities could do.
I try, but I would say I fail.
I mean, it's very difficult for human mind to really grapple with something
still completely alien.
I mean, even for us, right?
If we just try to imagine how would it feel if we were completely indifferent
towards death or individuality, even if you just imagine that, for example,
you could just copy my knowledge of how to speak Swedish.
Boom, now you can speak Swedish.
And you could copy any of my cool experiences and you could delete the one
you didn't like in your own life, just like that.
It would already change quite a lot about how you feel as a human being, right?
You probably spend less effort studying things if you just copy them and you
might be less afraid of death because if the plane you're on starts to crash,
you'd just be like, Oh, shucks, I'm going to, I had haven't backed my brain up for
four hours.
So I'm going to lose all this wonderful experiences on this flight.
We might also start feeling more like compassionate, maybe with other people.
If we can so readily share each other's experiences and our knowledge and feel
more like a hive mind, it's very hard though.
I really feel very humble about this to grapple with it, that the how it might
actually feel that the one thing which is so obvious though, which I think is
just really worth reflecting on is because the mind space,
the possible intelligence is so different from ours.
It's very dangerous if we assume they're going to be like us or anything like us.
Well, there's the entirety of human written history has been through poetry,
through novels, been trying to describe through philosophy,
trying to describe the human condition and what's entailed in it.
Like Jessica said, fear of death and all those kinds of things, what is love and
all of that changes.
If you have a different kind of intelligence, all of it, the entirety,
all those poems that are trying to sneak up to what the hell it means to be human.
All of that changes, how AI concerns and existential crisis is that AI
experiences how that clashes with the human existential crisis, the human condition.
Yeah.
That's hard to, hard to fathom, hard to predict.
It's hard, but it's fascinating to think about also, even in the best case scenario,
where we don't lose control of over the ever more powerful AI that we're building
to other humans whose goals we think are horrible and where we don't lose control
to the machines and AI provides the things we want.
Even then you get into questions, do you touch here?
You know, maybe it's the struggle that it's actually hard to do things is part
of the things that gives us meaning as well.
Right. So for example, I found it so shocking that this new Microsoft GPT
for commercial that they put together has this woman talking about showing this
demo of how she's going to give a graduation speech to her beloved daughter.
And she asks GPT for to write it.
It was freaking 200 words or so.
If I realized that my parents couldn't be bothered struggling a little bit to write
200 words and outsourced that to their computer, I would feel really offended,
actually. And so I wonder if I'm eliminating too much of this
struggle from our existence.
Do you think that would also take away a little bit of what means to be human?
Yeah.
Yeah.
We can't even predict.
I had somebody mentioned to me that they use, they started using Chad GPT with a 3.5
and that 4.0 to write what they really feel to a person.
And they have a temperature issue and they're basically trying to get Chad GPT
to rewrite it in a nicer way to get the point across, but rewrite it in a nicer way.
So we're even removing the inner asshole from our communication.
So I don't, you know, there's some positive aspects of that, but mostly it's just
the transformation of how humans communicate.
And it's scary because so much of our society is based on this glue of communication.
And if we're not using AI as the medium of communication that does the language for us,
so much of the emotion that's laden in human communication and so much of the
intent that's going to be handled by outsourced AI, how does that change everything?
How does it change the internal state of how we feel about other human beings?
What makes us lonely?
What makes us excited?
Yeah.
What makes us afraid and how we fall in love, all that kind of stuff.
Yeah.
For me personally, I have to confess the challenge is one of the things that really
makes my life feel meaningful.
You know, if I go hiking mountain with my wife, may I?
I don't want to just press a button and be at the top.
I want the struggle and come after sweating and feel, wow, we did this in the same way.
I want to constantly work on myself to become a better person.
If I say something in anger that I regret, I want to go back and really work on myself
rather than just tell an AI from now on always filter what I write.
So I don't have to work on myself because then I'm not growing.
Yeah, but then again, it could be like with chess and AI.
Once it's significantly, obviously, super sees the performance of humans.
It will live in its own world and provide maybe a flourishing
civilization for humans, but we humans will continue hiking mountains and playing
our games, even though AI is so much smarter, so much stronger, so much superior
in every single way, just like with chess.
Yeah.
So that that I mean, that's one possible hopeful trajectory here is that humans
will continue to human and AI will just be a kind of a medium that enables
the human experience to flourish.
Yeah.
I would phrase that as rebranding ourselves from Homo sapiens to Homo sentience.
You know, right now it's sapiens, the ability to be intelligence.
We've even put it in our species name.
So we're branding ourselves as a smartest.
Yeah.
Information processing.
Empty on the planet.
That's clearly going to change if AI continues ahead.
So maybe we should focus on the experience.
Instead, the subjective experience that we have with Homo sentience and say,
that's what's really valuable, the love, the connection, the other things.
And get off our high horses and get rid of this hubris that only we can do.
We do integrals.
So consciousness, the subjective experience is a fundamental value to what
it means to be human.
Make that make that the priority.
That feels like a hopeful direction to me, but that also requires more
compassion, not just towards other humans, because they happen to be the smartest
on the planet, but also towards all our other fellow creatures on this planet.
I personally feel right now we're treating a lot of farm animals horribly, for
example, and the excuse we're using is, oh, they're not as smart as us.
But if we get there, we're not that smart in the grand scheme of things, either
in the post AI epoch, you know, then surely we should value
the subjective experience of a cow also.
Well, allow me to briefly look at the book, which at this point is becoming more
and more visionary that you've written, I guess, over five years ago, life 3.0.
So first of all, 3.0, what's 1.0?
What's 2.0?
What's 3.0?
And how's that vision sort of evolve, the vision in the book evolved to today?
Life 1.0 is really dumb, like bacteria.
And that it can't actually learn anything at all during the lifetime.
The learning just comes from this genetic process from one generation to the next.
Life 2.0 is us and other animals, which have brains, which can learn during their
lifetime a great deal.
Right.
So.
And, you know, you were born without being able to speak English.
And at some point you decided, Hey, I want to upgrade my software.
Let's install an English speaking module.
So you did.
And life 3.0 does not exist yet.
It cannot replace not only its software the way we can, but also its hardware.
And that's where we're heading towards high speed.
We're already maybe 2.1 because we can, you know, put in an artificial knee,
pacemaker, etc, etc. And if New Orleans, you know, other companies succeed, we'll be life 2.2, etc.
But the companies trying to build a GI or trying to make is, of course, full 3.0.
And you can put that intelligence in something that also has no
biological basis whatsoever.
So less constraints and more capabilities, just like the leap from 1.0 to 2.0.
There is nevertheless you speaking so harshly about bacteria.
So disrespectfully about bacteria.
There is still the same kind of magic there that permeates life 2.0 and 3.0.
It seems like maybe the thing that's truly powerful about life, intelligence,
and consciousness was already there in 1.0.
Is it possible?
I think we should be humble and not be so quick to make everything binary and say,
either it's there or it's not.
Clearly there's a great spectrum and there is even controversy by whether some
unicellular organisms like amoebas can maybe learn a little bit.
You know, after all, so apologies if I offended you.
Yeah, it wasn't my intent.
It was more that I wanted to talk up how cool it is to actually have a brain.
Yeah, where you can learn dramatically within your lifetime.
Typical human.
And the higher up you get from 1.0 to 2.0, the more you become the captain of your own
ship, the master of your own destiny, and the less you become a slave to whatever
evolution gave you, right?
But upgrading your software, we can be so different from previous generations and
even from our parents, much more so than even a bacterium, you know, no offense to them.
And if you can also swap out your hardware and take any physical form you want,
of course, really, this guy is the limit.
Yeah, so the it accelerates the rate at which you can perform the computation
computation that determines your destiny.
Yeah, and I think it's worth commenting a bit on what you means in this context.
Also, if you swap things out a lot, right?
This is controversial, but my
current understanding is that, you know,
life is best thought of not as a bag of meat or even a bag of
elementary particles, but rather it is in as a system which can process information
and retain its own complexity, even though nature is always trying to mess it up.
So it's all about information processing and
that makes it a lot like something like a wave in the ocean, which is not its water molecules,
the water molecules bob up and down, but the wave moves forward.
It's an information pattern in the same way you Lex, you're not the same atoms as during the first
you did with me, you've swapped out most of them, but still you.
The information pattern is still there.
And if you could swap out your arms and whatever, you can still have this kind of continuity.
It becomes much more sophisticated sort of wave for the information lives on.
I lost both of my parents since our last podcast and it actually gives me a lot of solace
that this way of thinking about them.
They haven't entirely died because a lot of mommy and daddy's, sorry, I'm getting a little emotional
here, but a lot of their values and ideas and even jokes and so on, they haven't gone away.
Some of them live on, I can carry on some of them and they also live on a lot of other
and a lot of other people.
So in this sense, even with Life 2.0, we can to some extent already transcend our
physical bodies and our death.
And particularly if you can share your own information, your own ideas with many others,
like you do in your podcast, then that's the closest immortality we can get with our biobodies.
You carry a little bit of them and you and some sense.
Do you miss them?
You miss your mom and dad?
Of course.
Of course.
What did you learn about life from them if it can take a bit of a tangent?
I don't know so many things.
For starters, my fascination for math and the physical mysteries of our universe,
I think you got a lot of that from my dad, but I think my obsession for really big questions
and consciousness and so on, that actually came mostly from my mom.
What I got from both of them, which is very core part of who I am, I think is this...
Just feeling comfortable with
not buying into what everybody else is saying.
Doing what I think is right.
They both very much just did their own thing and sometimes they got flack for it and they did it anyway.
That's why you've always been an inspiration to me.
That you're at the top of your field and you're still willing to tackle the big questions in your
own way. One of the people that represents MIT best to me, you've always been an inspiration.
It's good to hear that you got that from your mom and dad.
Yeah, you're too kind.
The good reason to do science is because you're really curious and want to figure out the truth.
If you think this is how it is and everyone else says, "No, no, that's bullshit and it's that way."
You stick with what you think is true and even if everybody else keeps thinking it's bullshit,
there's a sort of... I always root for the underdog when I watch movies and my dad once,
one time for example when I wrote one of my craziest papers ever, I'm talking about our
universe ultimately being mathematical, which we're not going to get into today.
I got this email from a quite famous professor saying this is not only on bullshit, but it's
going to ruin your career. You should stop doing this kind of stuff. I sent it to my dad.
Do you know what he said? He replied with a quote from Dante,
"Segil to a corso, el acetir legende. Follow your own path and let the people talk. Go dad!"
This is the kind of thing. He's dead, but that attitude is not.
How did losing them as a man, as a human being, change you? How did it expand your thinking about
the world? How did it expand your thinking about this thing we're talking about, which is humans
creating another living sentient perhaps being? I think it mainly did two things.
One of them just going through all their stuff, they had passed away and so on,
just drove home to me how important it is to ask ourselves,
"Why are we doing this things we do?" Because it's inevitable that you look at some things they
spent an enormous time on and you ask them, "And hindsight, would they really have spent so much
time on this? Would they have done something that was more meaningful?"
So I've been looking more in my life now and asking, "Why am I doing what I'm doing?"
And I feel it should either be something I really enjoy doing or it should be something
that I find really, really meaningful because it helps humanity.
If it's none of those two categories, maybe I should spend less time on it.
The other thing is dealing with death up in person like this. It's actually made me
less afraid of the, even less afraid of other people telling me that I'm an idiot, which happens
regularly and just live my life, do my thing. It's made it a little bit easier for me to focus
on what I feel is really important. What about fear of your own death? Has it made it more real
that this is something that happens? Yeah, it's made it extremely real and I'm next in line in
our family now, right? It's me and my younger brother. But they both handled it such dignity.
That was a true inspiration also. They never complained about things and when you're old and
your body starts falling apart, it's more and more to complain about. They looked at what could
they still do that was meaningful and they focused on that rather than wasting time
talking about or even thinking much about things they were disappointed in.
I think anyone can make themselves depressed if they start their morning by making a list of
grievances. Whereas if you start your day with a little meditation and things you're grateful for,
you basically choose to be a happy person. Because you only have a finite number of days.
Yeah. You should spend them. Make your count. Being grateful.
Well, you do happen to be working on a thing which seems to have potentially
some of the greatest impact on human civilization of anything humans have ever created,
which is artificial intelligence. This is on the both detail technical level and in the high
philosophical level you work on. So you've mentioned to me that there's an open letter
that you're working on. It's actually going live in a few hours. I've been having late nights and
early mornings. It's been very exciting actually. In short, have you seen Don't Look Up? The film?
Yes, yes. I don't want to be the movie spoiler for anyone watching this who hasn't seen it.
But if you're watching this, you haven't seen it. Watch it. Because we are actually acting out.
It's life imitating art. Humanity is doing exactly that right now, except it's an asteroid that we
are building ourselves. Almost nobody is talking about it. People are squabbling across the planet
about all sorts of things which seem very minor compared to the asteroid that's about to hit us.
Most politicians don't even have this on the radar. They think maybe in 100 years or whatever.
Right now, we're at a fork on the road. This is the most important fork the humanity has reached
in its over 100,000 years on this planet. We're building effectively a new species that's smarter
than us. It doesn't look so much like a species yet because it's mostly not embodied in robots,
but that's a technicality which will soon be changed. This arrival of artificial general intelligence
that can do all our jobs as well as us and probably shortly thereafter, superintelligence
which greatly exceeds our cognitive abilities. It's going to either be the best thing ever to
happen, humanity or the worst. I'm really quite confident that there is not that much middle ground
there, but it would be fundamentally transformative to human civilizations.
Of course, utterly and totally. We branded ourselves as homo sapiens because it seemed like the
basic thing. We're the king of the castle on this planet. We're the smart ones. If we can control
everything else, this could very easily change. We're certainly not going to be the smartest
on the planet very long unless AI progress just halts. We can talk more about why
I think that's true because it's controversial. Then we can also talk about
reasons you might think it's going to be the best thing ever and the reason you think it's going
to be the end of humanity which is, of course, super controversial. What I think anyone who is
working on advanced AI can agree on is it's much like the film Don't Look Up and that
it's just really comical how little serious public debate there is about it. Given how huge it is.
So what we're talking about is a development of currently things like GPT-4
and the signs it's showing of rapid improvement that may in the near-term lead to development of
super intelligent AGI, general AI systems and what kind of impact that has on society.
When that thing achieves general human level intelligence and then beyond that general
superhuman level intelligence, there's a lot of questions to explore here. So one you mentioned
halt is that the content of the letter is to suggest that maybe we should pause the development of
these systems. Exactly. So this is very controversial. When we talked the first time,
we talked about how I was involved in starting the Future Life Institute and we worked very hard
on 2014-2015 was the mainstream AI safety. The idea that there even could be risks and that
you could do things about them. Before then, a lot of people thought it was just really
kooky to even talk about it and a lot of AI researchers felt
worried that this was too flaky and could be bad for funding and that the people who talked about
it were just not didn't understand AI. I'm very, very happy with how that's gone and that now,
you know, just completely mainstream, you go on any AI conference and people talk about AI safety
and it's a nerdy technical field full of equations and similar and blah, blah.
As it should be, but there is this other thing which has been quite taboo up until now calling
for slowdown. So what we've constant been saying, including myself, I've been biting my tongue a
lot, you know, is that, you know, we don't need to slow down AI development. We just need to win
this race, the wisdom race between the growing power of the AI and the growing wisdom with which
we manage it. And rather than trying to slow down AI, let's just try to accelerate the wisdom.
Do all this technical work to figure out how you can actually ensure that your powerful AI is
going to do what you wanted to do and have society adapt also with incentives and regulations so
that these things could put the good use. Sadly, Matt didn't pan out. The progress on technical AI
on capabilities has gone a lot faster than many people thought back when we started this in 2014.
Turned out to be easier to build really advanced AI than we thought.
And on the other side, it's gone much slower than we hoped with getting
policy makers and others to actually put in place incentives to make
steer this in the good direction. We can maybe we should unpack it and talk a little bit about
each. So why did it go faster than a lot of people thought? In hindsight, it's exactly like
building flying machines. People spent a lot of time wondering about how the birds fly.
And that turned out to be really hard. Have you seen the TED Talk with a flying bird?
Like a flying robotic bird? Yeah, it flies around the audience. But it took 100 years
longer to figure out how to do that than for the Wright brothers to build the first airplane.
Because it turned out there was a much easier way to fly. And evolution picked a more complicated
one because it had its hands tied. It could only build a machine that could assemble itself,
which the Wright brothers didn't care about. They can only build a machine that used only the
most common atoms in the periodic table. Wright brothers didn't care about that. They could use steel,
iron atoms. And it had to be able to repair itself. And it also had to be incredibly fuel
efficient. A lot of birds use less than half the fuel of a remote control plane flying
the same distance. For humans, throw a little more fuel in a roof. There you go, 100 years earlier.
That's exactly what's happening now with these large language models. The brain is incredibly
complicated. Many people made the mistake. You're thinking we have to figure out how the brain
does human level AI first before we could build in a machine that was completely wrong. You can
take an incredibly simple computational system called a transformer network and just train it
to do something incredibly dumb. Just read a gigantic amount of text and try to predict the
next word. And it turns out if you just throw a ton of compute at that, then a ton of data,
it gets to be frighteningly good, like GPT-4, which I've been playing with so much since it came out.
There's still some debate about whether that can get you all the way to full human level or not.
But we can come back to the details of that and how you might get the human level AI even if
a large language model is done. Can you briefly, if it's just a small tangent comment on your
feelings about GPT-4, suggest that you're impressed by this rate of progress? But where is it?
Can GPT-4 reason? What are the intuitions? What are human interpretable words you can assign
to the capabilities of GPT-4 that makes you so damn impressed with it?
I'm both very excited about it and terrified. It's an interesting mixture of emotions.
All the best things in life include those two somehow.
Yeah, it can absolutely reason. Anyone who hasn't played with it, I highly recommend doing that
before dissing it. It can do quite a remarkable reasoning. I've had to do a lot of things,
which I realized I couldn't do that myself that well even. And obviously, those are
dramatically faster than we do too when you watch a type. And it's doing that while servicing a
massive number of other humans at the same time. At the same time, it cannot reason as well as
a human can on some tasks. It's obviously a limitation from its architecture. We have,
in our heads, what in GeekSpeak is called a recurrent neural network,
there are loops. Information can go from this neuron to this neuron and then back to this one.
You can ruminate on something for a while. You can self reflect a lot. These large language models
that they cannot like GPT-4. It's a so-called transformer, where it's just like a one-way
street of information, basically. In GeekSpeak, it's called a feed-forward neural network.
And it's only so deep. So it can only do logic that's that many steps and that deep.
And you can create problems which will fail to solve for that reason.
But the fact that it can do so amazing things with this incredibly simple architecture already
is quite stunning. And what we see in my lab at MIT, when we look inside
large language models to try to figure out how they're doing it, which that's the key core
focus of our research. It's called mechanistic interpretability in GeekSpeak. You have this
machine that does something smart to try to reverse engineer. See how does it do it?
I think of it also as artificial neuroscience.
Exactly what neuroscientists do with actual brains. But here you have the advantage that you can,
you don't have to worry about measurement errors. You can see what every neuron is doing all the time.
And the recurrent thing we see again and again, there's been a number of beautiful papers quite
recently by a lot of researchers. Some of them here in this area is where when they figure out
how something is done, you say, "Oh man, that's such a dumb way of doing it." And you immediately
see how it can be improved. For example, there was a beautiful paper recently where they figured out
how a large language model stores certain facts like Eiffel Tower is in Paris. And they figured out
exactly how it's stored. The proof that they understood it was they could edit it. They changed
some of the synapses in it. And then they asked it, "Where's the Eiffel Tower?" And they said,
"It's in Rome." And then they asked, "How do you get there?" "Oh, how do you get there from Germany?"
"Oh, you take this train, the Roma Teremini train station and this and that. And what might you see
if you're in front of it?" "Oh, you might see the Colosseum." So they had edited it. So they literally
moved it to Rome. But the way that it's storing this information, it's incredibly dumb for
any fellow nerds listening to this. There was a big matrix. And roughly speaking,
there are certain row and column vectors which encode these things and the corresponding very
hand-wave elite to principal components. And it would be much more efficient for sparse matrix
just storing the database. And everything so far, we figured out how these things do.
Our ways are you can see they can easily be improved. And the fact that this particular
architecture has some roadblocks built into it is in no way going to prevent crafty researchers
from quickly finding workarounds and making other kinds of architectures go all the way.
So it's in short, it's turned out to be a lot easier to build close to human intelligence than
we thought. And that means our runway as a species to get our shit together has shortened.
And it seems like the scary thing about the effectiveness of large language models.
So Sam Altman, every single conversation with, and he really showed that the leap from GPT-3
to GPT-4 has to do with just a bunch of hacks. A bunch of little explorations, but with a
smart researchers doing a few little fixes here and there. So that's some fundamental leap
and transformation in the architecture. And more data and more compute.
And more data and compute, but he said the big leaps has to do with not the data in the compute,
but just learning this new discipline, just like you said. So researchers are going to look at
these architectures and there might be big leaps where you realize, wait, why are we doing this in
this dumb way? And all of a sudden this model is 10x smarter. And that can happen on any one day,
on any one Tuesday or Wednesday afternoon. And then all of a sudden you have a system that's 10x
smarter. It seems like it's such a new discipline. It's such a new, like we understand so little
about why this thing works so damn well, that the linear improvement of compute or exponential,
but the steady improvement of compute, steady improvement of the data may not be the thing
that even leads to the next leap. It could be a surprise little hack that improves everything.
There are a lot of little leaps here and there because so much of this is out in the open also.
So many smart people are looking at this and trying to figure out little leaps here and there.
And it becomes this sort of collective race where if a lot of people feel if I don't take the leap
someone else with. And it is actually very crucial for the other part of it. Why do we want to slow
this down? So again, what this open letter is calling for is just pausing all training
of systems that are more powerful than GPT for for six months. Give a chance
for the labs to coordinate a bit on safety and for society to adapt, give the right incentives
to the labs. Because you've interviewed a lot of these people who lead these labs and you know,
just as well as I do, they're good people. They're idealistic people. They're doing this
first and foremost because they believe that AI has a huge potential to help humanity.
But at the same time, they are trapped in this horrible race to the bottom.
Have you read Meditations on Moloch by Scott Alexander?
Yes. Yeah, it's a beautiful essay on this poem by Ginsburg where he interprets it as being about
this monster. It's this game theory monster that pits people against each other in this race,
the bottom where everybody ultimately loses it, the evil thing about this monster is even
though everybody sees it and understands they still can't get out of the race.
Most a good fraction of all the bad things that we humans do are caused by Moloch. And I like
Scott Alexander's naming of the monster so we humans can think of it as an "if" thing.
If you look at why do we have overfishing, why do we have more generally the tragedy of the commons,
why is it that to live in an array? I don't know if you've had her on your podcast.
Yeah, she's become a friend.
Great. She made this awesome point recently that beauty filters that a lot of female
influencers feel pressured to use are exactly Moloch in action again. First, nobody was using them
and people saw them just the way they were. And then some of them started using it
and becoming ever more plastic. Fantastic. And then the other ones that weren't using it started
to realize that if they want to just keep their market share, they have to start using it too.
And then you're in the situation where they're all using it. And none of them has any more market
share or less than before. So nobody gained anything. Everybody lost. And they have to keep
becoming ever more plastic. Fantastic also. But nobody can go back to the old way because it's just
too costly. Moloch is everywhere. And Moloch is not a new arrival on the scene.
Either. We humans have developed a lot of collaboration mechanisms to help us fight back
against Moloch through various kinds of constructive collaboration. The Soviet Union and the United
States did sign the number of arms control treaties against Moloch who is trying to stoke them into
unnecessarily risky nuclear arms races, et cetera, et cetera. And this is exactly what's happening
on the AI front. This time, it's a little bit geopolitics, but it's mostly money,
where there's just so much commercial pressure. If you take any of these
leaders of the top tech companies, if they just say, "This too risky, I want to pause
for six months," they're going to get a lot of pressure from shareholders and others.
We're like, "Well, if you pause, but those guys don't pause, we're... If you don't want to get
our lunch eaten, shareholders even have a power to replace the executives in the worst case."
We did this open letter because we want to help these idealistic tech executives to do
what their heart tells them by providing enough public pressure on the whole sector.
Just pause so that they can all pause in a coordinated fashion. And I think without the public pressure,
none of them can do it alone, push back against their shareholders, no matter how good-hearted they
are. Moloch is a really powerful foe. So the idea is to, for the major developers of AI systems
like this, so we're talking about Microsoft, Google, Meta, and anyone else.
Well, OpenAI is very close with Microsoft now, and there are plenty of smaller players.
For example, Anthropic is very impressive. There's conjecture. There's many, many players.
I don't want to make a long list to leave anyone out. And for that reason, it's so important that
some coordination happens, that there's external pressure on all of them saying you all need the
pause. Because then the researchers in, these organizations, the leaders who want to slow down
a little bit, they can say they're shareholders. Everybody's slowing down because of this pressure,
and it's the right thing to do. Have you seen in history, their examples, what's possible to pause
the Moloch? Absolutely. And even like human cloning, for example, you could make so much money on
human cloning. Why aren't we doing it? Because biologists thought hard about this, and
felt like this is way too risky. They got together in the 70s in the cello mare and decided even
to stop a lot more stuff, also just editing the human germline, gene editing that goes in
to our offspring. And decided let's not do this, because it's too unpredictable what it's going to
lead to. We could lose control over what happens to our species, so they paused. There was a ton
of money to be made there. So it's very doable. But you just need a public awareness of what the
risks are, and the broader community coming in and saying, hey, let's slow down. And another
common pushback I get today is we can't stop in the West because China and in China and out of
Libby, they also get told we can't slow down because the West, because both sides think they're the good
guy. But look at human cloning. Did China forge ahead with human cloning? There's been exactly one
human cloning that's actually been done that I know of. It was done by Chinese guy. Do you know where
he is now? In jail. And you know who put him there? Who? Chinese government. Not because
Westerners said China, look, this is, no, the Chinese government put him there because they also felt
they like control Chinese government. If anything, maybe they are even more concerned about having
control than Western governments have no incentive of just losing control over where everything is
going. And you can also see the Ernie bot that was released, I believe, by do recently. They got a
lot of pushback from the government and had to rein it in, you know, in a big way. I think once
this basic message comes out that this isn't an arms race, it's a suicide race, where everybody
loses if anybody's AI goes out of control. It really changes the whole dynamic. It's not,
I'll say this again, because this is this very basic point I think a lot of people get wrong.
Because a lot of people dismiss the whole idea that AI can really get very superhuman,
because they think there's something really magical about intelligence such that it can only exist
new in mind, because they believe that. I think it's going to kind of get to just more or less
GPT four plus plus, and then that's it. They don't see it as a super, as a suicide race. They
think whoever gets that first, they're going to control the world, they're going to win.
That's not how it's going to be. And we can talk again about the scientific arguments from why
it's not going to stop there. But the way it's going to be is if anybody completely loses control,
and you don't care if someone manages to take over the world who really doesn't share your goals,
you probably don't really even care very much about what nationality they have. You're not going
to like it. It's much worse than today. If you live in Orwellian dystopia, what do you care
who created it? And if it goes farther, and we just lose control even to the machines,
so that it's not us versus them, it's us versus it. What do you care who created this
underlying entity, which has goals different from humans ultimately, and we get marginalized,
we get made obsolete, we get replaced. That's why what I mean when I say it's a suicide race.
It's kind of like we're rushing towards this cliff, but the closer the cliff we get, the more
scenic the views are, and the more money there is there, so we keep going. But we have to also
stop at some point, right? Well, we're ahead. And it's a suicide race, which cannot be won,
but the way to really benefit from it is to continue developing awesome AI a little bit slower,
so we make it safe, make sure it does the things that humans want, and create a condition where
everybody wins. The technology has shown us that geopolitics and politics in general is not a zero
sum game at all. So there is some rate of development that will lead us as a human species to lose
control of this thing. And the hope you have is that there's some lower level of development,
which will not allow us to lose control. This is an interesting thought you have about losing
control. So if you're somebody like Sunup or Chai or Sam Altman at the head of a company like this,
you're saying if they develop an AGI, they too will lose control of it. So no one person can
maintain control, no group of individuals can maintain control. If it's created very, very soon,
and is a big black box that we don't understand, like the large language models, yeah, then I'm
very confident they're going to lose control. But this isn't just me saying it, you know,
Sam Altman and Dennis Asabez have both said themselves, acknowledge that there's really
great risks with this, and they want to slow down once they feel it gets scary. But it's clear that
they're stuck in this, again, Molok is forcing them to go a little faster than they're comfortable
with because of pressure from just commercial pressures, right? To get a bit optimistic here,
of course, this is a problem that can be ultimately solved. It just to win this wisdom race,
it's clear that what we hope that it was going to happen hasn't happened. The capability progress
has gone faster than a lot of people thought, and the progress in the public sphere of policy
making and so on has gone slower than we thought. Even the technical AI safety has gone slower.
A lot of the technical safety research was kind of banking on that large language models and other
poorly understood systems couldn't get us all the way, but you had to build more of a kind of
intelligence that you could understand. Maybe it could prove itself safe, you know, things like this.
And I'm quite confident that this can be done so we can reap all the benefits, but we cannot do it
as quickly as this out of control express train we are on now as you know, just the AGI. That's
why we need it a little more time, I feel. Is there something to be said, what like Sam
Almond talked about, which is while we're in the pre-AGI stage to release often and as
transparently as possible to learn a lot? So as opposed to being extremely cautious, release a lot.
Don't invest in a closed development where you focus on AI safety while it's somewhat dumb,
quote unquote, release as often as possible. And as you start to see signs of
human level intelligence or superhuman level intelligence, then you put a halt on it.
Well, what a lot of safety researchers have been saying for many years is that the most
dangerous things you can do with an AI is first of all, teach it to write code.
Yeah, because that's the first step towards recursive self improvement, which can take it from
AGI to much higher levels. Okay, oops, we've done that. And another thing, high risk is connected
to the internet. Let it go to websites, download stuff on its own, talk to people. Oops, we've done
that already. You know, L.E.S. you, Kowski, you said you interviewed him recently, right?
Yeah, he had this tweet recently, which I got giving one of the best laughs in a while,
where he was like, hey, people used to make fun of me and say, you're so stupid,
Ellie, as you're saying, you're saying, you have to worry of obviously developers once they get
to like really strong AI. First thing you're going to do is like never connect it to the
internet, keep it in the box, where you know, you can really study it. So he had written it in
the like in the meme form. So it's like then. Yeah, and then that. And now let's let's make it
chatbot. Yeah. And the third thing is Stuart Russell. Yeah, you know, amazing AI researcher.
He has argued for a while that we should never teach AI anything about humans.
Above all, we should never let it learn about human psychology and how you manipulate humans.
That's the most dangerous kind of knowledge you can give it. Yeah, you can teach it all it
needs to know how to about how to cure cancer and stuff like that. But don't let it read Daniel
Kahneman's book about cognitive biases and all that. And then, oops, L.O.L. you know, let's
invent social media. I'll recommend our algorithms, which do exactly that. They get so good at knowing
us and pressing our buttons that we've we're starting to create a world now where we just have
ever more hatred because they figured out that these algorithms not for out of evil, but just to
make money on advertising that the best way to get more engagement with euphemism get people glued
to their little rectangles. It is just to make them pissed off. That's really interesting that
a large AI system that's doing the recommender system kind of task on social media is basically
just studying human beings because it's a bunch of us rats giving it signal, nonstop signal.
It'll show a thing and then we give signal and whether we spread that thing, we like that thing,
that thing increases our engagement, gets us to return to the platform. It has that on the scale
of hundreds of millions of people constantly. So it's just learning and learning and learning.
And presumably if the number of parameters in neural network that's doing the learning,
a more end-to-end the learning is the more it's able to just basically encode how to manipulate
human behavior, how to control humans at scale. Exactly. And that is not something you think is
a new man in his interest. Yes. Right now it's mainly letting some humans manipulate other humans for
power, which already caused a lot of damage. And eventually that's a sort of
skill that can make AI's persuade humans to let them escape whatever safety precautions we have.
But there was a really nice article in the New York Times recently by Yval Noah Harari and two
co-authors including Tristan Harris from the social dilemma. We have this phrase in there,
I love. "humanity's first contact with advanced AI was social media and we lost that one.
We now live in a country where there's much more hate in the world where there's much more hate
in fact. And in our democracy that we're having this conversation and people can't even agree on
who won the last election, you know. And we humans often point fingers at other humans and say it's
their fault. But it's really malloc and these AI algorithms. We got the algorithms and then malloc
pitted the social media companies against each other so nobody could have a less creepy algorithm
because then they would lose out on our revenue to the other company. Is there any way to win that
battle back just if we just linger on this one battle that we've lost in terms of social media?
Is it possible to redesign social media this very medium in which we use as a civilization
to communicate with each other, to have these kinds of conversations, to have this course to try to
figure out how to solve the biggest problems in the world whether that's nuclear war or
the development of AGI. Is it possible to do social media correctly?
I think it's not only possible but it's necessary. Who are we kidding that we're going to be able
to solve all these other challenges if we can't even have a conversation with each other. It's
constructive. The whole idea, the key idea of democracy is that you get a bunch of people together
and they have a real conversation. The ones you try to foster on this podcast
where you respectfully listen to people you disagree with and you realize actually there are some
things actually some common ground we have and we both agree let's not have a nuclear war, let's
not do that etc. We're kidding ourselves thinking we can face off the second contact with ever
more powerfully. That's happening now with these large language models if we can't even
have a functional conversation in the public space. That's why I started to improve the news
project, improve the news data work. But I'm an optimist fundamentally in
that there is a lot of intrinsic goodness in people and that what makes the difference
between someone doing good things for humanity and bad things is not some sort of fairy tale
thing that this person was born with the evil gene and this one was born with a good gene. No,
I think it's whether we put for the people find themselves in situations that bring out the best
in them or they bring out the worst in them. I feel we're building an internet and a society that
brings out the worst. But it doesn't have to be that way. No, it does not. It's possible to create
incentives and also create incentives that make money. They both make money and bring out the
best in people. I mean in the long term it's not a good investment for anyone to have a nuclear
war for example. And is it a good investment for humanity if we just ultimately replace all humans
by machines and then we're so obsolete that eventually there's no humans left? Well, depends
against how you do the math. But I would say by any reasonable economic standard if you look at
the future income of humans and there aren't any, that's not a good investment. More over like,
why can't we have a little bit of pride in our species? Why should we just build another species
that gets rid of us? If we were Neanderthals, would we really consider it a smart move if
we had really advanced biotech to build Homo sapiens? You might say, hey Max, let's build
these Homo sapiens. They're going to be smarter than us. Maybe they can help us defend us better
against predators and help fix up our caves, make them nicer. We'll control them undoubtedly.
So then they build a couple, a little baby girl, little baby boy. And then you have some
wise old, then the Neanderthal elder is like, hmm, I'm scared that we're opening a Pandora's box
here and that we're going to get outsmarted by these super Neanderthal intelligences and
they won't be in Neanderthals left. But then you have a bunch of others in the cave, you are,
you're such a Luddite scaremonger, of course, they're going to want to keep us around because
we are their creators and the smarter, I think the smarter they get, the nicer they're going to get,
they're going to leave us, they're going to want us around and it's going to be fine. And besides,
look at these babies, they're so cute. Clearly, they're totally harmless. Those babies are
exactly GPT-4. I want to be clear. It's not GPT-4 that's terrifying. It's the GPT-4 is a baby
technology. Microsoft even had a paper recently out,
the title something like "Sparkles of AGI." Well, they were basically saying this is baby AI,
like these little Neanderthal babies. And it's going to grow up. There's going to be other systems
from the same company, from other companies. They'll be way more powerful, but they're going to take
all the things, ideas from these babies. And before we know it, we're going to be like
those last Neanderthals who are pretty disappointed when they realized that they were getting replaced.
Well, this interesting point you make, which is a programming, it's entirely possible that GPT-4
is already the kind of system that can change everything by writing programs.
Yeah, it's because it's life 2.0. The systems I'm afraid of are going to look nothing like a
large language model. But once it gets, once it or other people figure out a way of using this
tech to make much better tech, it's just constantly replacing its software. And from everything we've
seen about how these work under the hood, they're like the minimum viable intelligence. They do
everything in a dumbest way that still works. So they are life 3.0, except when they replace the
software, it's a lot faster than when you decide to learn Swedish. And moreover, they think a lot
faster than us too. So when we don't think of one logical step every nanosecond or few or so,
the way they do, and we can't also just suddenly scale up our hardware massively in the cloud,
which is so limited. So they are also life consumed to become a little bit more like life 3.0,
and if they need more hardware, hey, just rent it in the cloud. How do you pay for it? Well,
with all the services you provide.
And what we haven't seen yet, which could change a lot, is an entire software system. So right now,
programming is done in bits and pieces as an assistant tool to humans. But I do a lot of programming,
and with the kind of stuff that GPT-4 is able to do, I mean, is replacing a lot what I'm able to
do. But you still need a human in the loop to kind of manage the design of things, manage like
what are the prompts that generate the kind of stuff, to do some basic adjustment of the code,
to do some debugging. But if it's possible to add on top of GPT-4 kind of feedback loop of
self-debugging, improving the code, and then you launch that system out into the wild on the
internet because everything is connected and have it do things, have it interact with humans,
and then get that feedback. Now you have this giant ecosystem of humans. It's one of the things that
Elon Musk recently sort of tweeted as a case why everyone needs to pay $7 or whatever for Twitter.
To make sure they're real.
Then make sure they're real. We're now going to be living in a world where the bots are getting
smarter and smarter and smarter to a degree where you can't tell the difference between a human
and a bot. And now you can have bots outnumber of humans by 1 million to one, which is why
he's making the case, why you have to pay to prove you're human, which is one of the only
mechanisms to prove, which is depressing.
And I feel we have to remember as individuals, we should from time to time ask ourselves,
why are we doing what we're doing? As a species, we need to do that too.
So if we're building, as you say, machines that are outnumbering us and more and more
outsmarting us and replacing us on the job market, not just for the dangerous and boring tasks,
but also for writing poems and doing art and things that a lot of people find really meaningful,
God ask ourselves, why? Why are we doing this?
The answer is malloc is tricking us into doing it. And it's such a clever trick that even though
we see the trick, we still have no choice but to fall for it. Also, the thing you said about you
using co-pilot AI tools to program faster, what factor faster would you say you code now?
This is called twice as faster. I don't really, because it's a new tool. I don't know if speed is
significantly improved, but it feels like I'm a year away from being 5 to 10 times faster.
So if that's typical for programmers, then you're already seeing another kind of self,
of course, of self-improvement. Previously, one, a major generation of improvement of the code
would happen on the human R&D timescale. Now, if that's five times shorter,
then it's going to take five times less time than otherwise would to develop the next level
of these tools and so on. So this is exactly the beginning of an intelligence explosion.
There can be humans in the loop a lot in the early stages, and then eventually,
humans are needed less and less and the machines can more go long. What you said there is just an
example of these sort of things. Another thing which I was kind of lying on my
psychiatrist, imagining I'm on a psychiatrist's couch here saying, "What are my fears that people
would do with AI systems?" So I mentioned three that I had fears about many years ago that they
would do, namely, teach the code connected to the internet and teach it to manipulate humans.
A fourth one is building an API where code can control the super powerful thing, right? That's
very unfortunate because one thing that systems like GPT-4 have going for them is that they are
an oracle in the sense that they just answer questions. There is no robot connected to GPT-4.
GPT-4 can't go and do stock trading based on its thinking. It is not an agent. An intelligent
agent is something that takes in information from the world, processes it, to figure out what
action to take based on its goals that it has, and then does something back on the world.
But once you have an API, for example, GPT-4, nothing stops Joe Schmoe and a lot of other people
from building real agents, which just keep making calls somewhere in some interloop somewhere
to these powerful oracle systems, which makes them themselves much more powerful. That's another
kind of unfortunate development which I think we would have been better off delaying.
I don't want to pick on any particular companies. I think they're all under a lot of pressure
to make money. Again, the reason we're calling for this pause is to give them all cover to do what
they know is the right thing. Slow down a little bit at this point. But everything we've talked about,
I hope, will make it clear to people watching this, why
these human-level tools can cause gradual acceleration. You keep using yesterday's technology to build
tomorrow's technology. When you do that over and over again, you naturally get an explosion.
That's the definition of an explosion in science. If you have two people, they fall in love,
now you have four people, and then they can make more babies, and now you have eight people,
and then you have 16, 32, 64, etc. We call that a population explosion,
whereas if it's instead free neutrons in a nuclear reaction, if each one can make
more than one, then you get an exponentially growth in that. We call it a nuclear explosion.
All explosions are like that. In an intelligence explosion, it's just exactly the same principle
that some amount of intelligence can make more intelligence than that. And then repeat,
you always get the exponentials. What's your intuition? Why does you mention there's some
technical reasons why it doesn't stop at a certain point? What's your intuition? Do you have any
intuition? Why it might stop? It's obviously going to stop when it bumps up against the laws of physics.
There are some things you just can't do, no matter how smart you are. Allegedly.
And because we don't have the full laws of physics. Yeah, right. Seth Lloyd wrote a really
cool paper on the physical limits on computation, for example. If you make it put too much energy
into it, then the finite space will turn into a black hole. You can't move information around
fast enough speed of light, stuff like that. But it's hard to store way more than the
modest number of bits per atom, etc. But those limits are just astronomically above, like 30 orders
of magnitude above where we are now. So bigger, different, bigger jumping intelligence than if you
go from an ant to a human. I think, of course, what we want to do is have a controlled
thing. A nuclear reactor, you put moderators in to make sure exactly it doesn't blow up out of
control. When we do experiments with biology and cells and so on, we also try to make sure it
doesn't get out of control. We can do this with AI too. The thing is, we haven't succeeded yet.
Molok is exactly doing the opposite, just fueling, just egging everybody on faster,
faster, faster, or the other company is going to catch up with you, or the other country is going
to catch up with you. We have to want this stuff. I don't believe in this, just asking people to
look into their hearts and do the right thing. It's easier for others to say that. But if you're
in the situation where your company is going to get screwed, by other companies, they're not
stopping, you're putting people in a very hard situation. The right thing to do is change the
whole incentive structure instead. Maybe I should say one more thing about this, because Molok
has been around as humanity is number one or number two enemy since the beginning of civilization.
We came up with some really cool countermeasures. First of all, already over 100,000 years ago,
evolution realized that it was very unhelpful that people kept killing each other all the time.
It genetically gave us compassion and made it so that if you get two drunk dudes getting into a
pointless bar fight, they might give each other black eyes, but they have a lot of inhibition
towards killing each other. Similarly, if you find a baby lying on the street when you go out
for your morning jog tomorrow, you're going to stop and pick it up. Even though it may be
a make you late for your next podcast. Evolution gave us the genes that make our own
egoistic incentives more aligned with what's good for the greater group or part of.
And then as we got a bit more sophisticated and developed language, we invented gossip,
which is also a fantastic anti-Molok. Now, it really discourages liars,
mooches, cheaters, because their own incentive now is not to do this, because word quickly gets
around and then suddenly people aren't going to invite them to their dinners anymore or trust
them. And then when we got still more sophisticated and bigger societies, we invented the legal system
where even strangers who couldn't rely on gossip and things like this would treat each other,
would have an incentive. Now those guys in the bar fights, even if someone is so drunk that he
actually wants to kill the other guy, he also has a little thought on the back of his head that,
you know, do I really want to spend the next 10 years eating like really crappy food in a small
room? I'm just going to chill out, you know? And we similarly have tried to give these incentives
to our corporations by having regulation and all sorts of oversight so that their incentives are
aligned with the greater good. We tried really hard. And the big problem that we're failing now
is not that we haven't tried before, but it's just that the tech is growing much
is developing much faster than the regulators been able to keep up, right? So
regulators, it's kind of comical, like European Union right now is doing this AI act, right?
Which, and in the beginning, they had a little opt out exception that GPT-4 would be completely
excluded from regulation. Brilliant idea. What's the logic behind that? Some lobbyists
pushed successfully for this. So we were actually quite involved with the Future Life Institute,
Mark Braco, Mr. Ook, Anthony Aguirre, and others, you know, we're quite involved with
talking to very educating various people involved in this process about
these general purpose AI models coming and pointing out that they would become the
laughing stock if they didn't put it in. So the French started pushing for it, it got put in
to the draft and it looked like all was good. And then there was a huge counter push from lobbyists.
Yeah, there were more lobbyists and Brussels from tech companies and from oil companies, for example.
And it looked like it might, is we're going to maybe get taken out again. And now GPT-4 happened.
And I think it's going to stay in. But this just shows, you know, malloc can be defeated.
But the challenge we're facing is that the tech is generally much faster than what the
policymakers are. And a lot of the policymakers also don't have a tech background. So it's,
you know, we really need to work hard to educate them on what's taking place here.
So we're getting this situation where the first kind of non... So I define artificial
intelligence just as non-biological intelligence. And by that definition, a company, a corporation
is also an artificial intelligence because the corporation isn't its humans, it's the system.
If a CEO decides... The CEO of a tobacco company decides one morning that
CEO, he doesn't want to sell cigarettes anymore. They'll just put another CEO in there.
It's not enough to align the incentives of individual people or align individual
computers incentives to their owners, which is what technically I safety research is about.
You also have to align the incentives corporations with a greater good. And some corporations have
gotten so big and so powerful very quickly that in many cases, they're lobbyists instead align
the regulators to what they want rather than the other way around the classic regulatory capture.
All right. Is the thing that the slowdown hopes to achieve is give enough time to the
regulators to catch up or enough time to the companies themselves to breathe and understand
how to do AI safety correctly? I think both. And I think that the vision of the path to success
I see is first you give a breather actually to the people in these companies. Their leadership
wants to do the right thing and they all have safety teams and so on on their companies.
Give them a chance to get together with the other companies. And the outside pressure can
also help catalyze that right and and work out what is it that's what are the reasonable
safety requirements one should put on future systems before they get rolled out. There are a
lot of people also in academia and elsewhere outside of these companies who can be brought
into this and have a lot of very good ideas. And then I think it's very realistic that within
six months you can get these people coming up to here's a white paper here is where we all think
it's reasonable. You know you didn't just because cars killed a lot of people he didn't ban cars
but they got together a bunch of people and decided you know in order to be allowed to sell a car
it has to have a seat belt in them. They're the analogous things that you can start requiring
a future AI systems so that they are safe. And once this heavy lifting this intellectual work has
been done by experts in the field which can be done quickly I think it's been going to be quite
easy to get policymakers to see yeah this is a good idea and it's you know for the companies to
fight malloc they want and I believe Sam Altman has explicitly called for this they want the
regulators to actually adopt it so that their competition is going to abide too right you don't
want you don't want to be in acting all these principles then you abide by them and then there's
this one little company that doesn't sign on to it and then now they can gradually overtake you
then the companies will get be able to sleep secure knowing that everybody is playing by the
same rules. So do you think it's possible to develop guardrails that keep the systems from
from basically damaging irreparably humanity while still enabling sort of the capitalist
fueled competition between companies as they develop how to best make money with this AI.
You think there's a balancing that's possible absolutely we've seen that and many other sectors
where you've had the free market produce quite good things without causing particular harm
when the guardrails are there and they work you know capitalism is a very good way of
optimizing for just getting the same things on more efficiently but it was good you know then
like in hindsight and I never met anyone even on parties way over on the right in any country
who think it was a bad it thinks it was a terrible idea to ban child labor for example.
Yeah but it seems like this particular technology has gotten so good so fast become
powerful to a degree where you could see in the near term the ability to make a lot of money
and to put guardrails develop guardrails quickly in that kind of contact seems to be tricky it's not
similar to cars or child labor it seems like the opportunity to make a lot of money here
very quickly is right here before us again there's this cliff
yeah it gets quite scenic closer to the cliff so you go yeah there more
there more money there is more gold ingots there on the ground you can pick up whatever you want
to drive there very fast but it's not in anyone's incentive that we go over the cliff and it's not
like everybody's in the wrong car all the cars are connected together with a chain yeah so if
anyone goes over they'll start dragging others down the others down too and so ultimately it's in
the selfish interests also of the people in the companies to to slow down when the when you just
start seeing the corners of the cliff there in front of you right and the problem is that even
though the people who are building the technology and the CEOs they really get it the shareholders and
these other market forces they are people who don't honestly
understand that the cliff is there they usually don't you have to get quite into the weeds
it's really appreciate how powerful this is and how fast and a lot of people are even still stuck
again in this idea that intelligence in this carbon chauvinism as i like to call it that you
can only have our level of intelligence in humans that there's something magical about it whereas
the people in the tech companies who build this stuff they all realize that intelligence is
information processing of a certain kind and it really doesn't matter at all whether the
information is processed by carbon atoms in neurons in brains or by silicon atoms in some
technology we build so you brought up capitalism earlier and there are a lot of people who love
capitalism and a lot of people who really really don't and it's struck me recently that the
what's happening with capitalism here is exactly analogous to the way in which super intelligence
might wipe us out so you know i studied economics from my undergrad stock school economics yay
well no i tell them so i was very interested in how how you could use market forces to just get
stuff done more efficiently but give the right incentives to market so that it wouldn't do really
bad things so Dylan had fieldman l who's a professor and colleague of mine at mit wrote this
really interesting paper with some collaborators recently where they proved mathematically that
if you just opt take one goal that you just optimized for on and on and on indefinitely
do you think it's gonna bring you in the right direction what basically always happens is in the
beginning it will make things better for you but if you keep going at some point it's going to
start making things worse for you again and then gradually it's going to make it really really
terrible so just as a simple the way i think of the proof is
suppose you want to go from here back to austin for example and you're like okay yeah let's just
let's go south but you put in exactly the right sort of the right direction
just optimize that south it's possible you get closer and closer to austin
but uh you there's always some little error so you you're not going exactly towards austin but
you get pretty close but eventually you start going away again and eventually you're going to
be leaving the solar system yeah and they they proved it's a beautiful mathematical proof this
happens generally and this is very important for ai because for even though steward russell has
written a book and given a lot of talks on why it's a bad idea to have ai just blindly optimize
something that's what pretty much all our systems do yeah we have something called the loss function
that we're just minimizing or reward function we're just maximize maximizing stuff and um
capitalism is exactly like that too we want we wanted to get stuff done more efficiently
than people wanted so you introduce the free market things got done much more efficiently than
they did and say communism right and it got better but then it just kept optimizing it
and kept optimizing and you got ever bigger companies and ever more efficient information
processing and now also very much powered by it and eventually a lot of people are beginning
to feel wait we're kind of optimizing a bit too much like why did we just chop down half the rain
forest you know and why why did suddenly these regulators get captured by lobbyists and so on
it's just the same optimization that's been running for too long if you have an ai that actually has
power over the world and you just give it one goal and just like keep optimizing that most likely
everybody's gonna be like yay this is great in the beginning things are getting better
but um it's almost impossible to give it exactly the right direction to optimize in and then
eventually all i hate breaks loose right nick bostrom and others are given example to sound
quite silly like what if you just want to like tell it to cure cancer or something and that's all
you tell it maybe it's gonna decide to take over entire continents just so we can get more super
computer facilities in there and figure out a cure cancer backwards and then you're like wait
that's not what i want anyway and um the the the issue with capitalism and the issue with the
front of the way i have kind of merged now because that mollock i talked about is exactly the capitalist
mollock that we have built an economy that has optimized for only one thing profit right and
that worked great back when things were very inefficient and then now it's getting them better
and it worked great as long as the companies were small enough that they couldn't capture the
regulators but that's not true anymore but they keep optimizing and now we they realize that
that they can these companies can make even more profit by building ever more powerful ai even if
it's reckless but optimize more more more more more more so this is mollock again showing up and i
just want to anyone here who has any concerns about about uh late stage capitalism having gone a
little too far you should worry about super intelligence because it's the same villain in both cases
it's mollock and optimizing one objective function aggressively blindly is going to take us there
yeah we have this pause from time to time and look into our hearts and that's why are we doing this
is this am i still going towards austin or have i gone too far you know maybe we should change
direction and that uh is the idea behind the halt for six months why six months that seems like a
very short period just can we just linger and explore different ideas here because this feels like a
really important moment in human history where a pausing would actually have a significant positive
effect we said six months because we figured the number one pushback we were going to get in the
west was like but china and uh everybody knows there's no way that china is going to catch up with
the west on this in six months so it's that argument goes off the table and you can forget about
geopolitical competition and just focus on the real issue that's why we put this that's really
interesting but you've already made the case that uh even for china if you actually want to take on
that argument china too would not be bothered by a longer halt because they don't want to lose
control even more than the west doesn't that's what i think that's a really interesting argument
like i have to actually really think about that which the the kind of thing people assume is if
you develop an AGI that open AI if they're the ones that do it for example they're going to win
but you're saying no they're everybody loses yeah it's gonna get better and better and better and
then kaboom we all lose that's what's gonna happen when lose and win a defined on a metric of
basically quality of life for human civilization and for sam oatman
i to be v blun my personal guess you know and people think quibble with this is that we're just
gonna there won't be any humans that's it that's what i mean by lose you know if you we can see in
history once you have some species or some group of people who aren't needed anymore
doesn't usually work out so well for them right yeah there were a lot of horses for the
were used for traffic in boston and then the car got invented and most of them got yeah well
we don't need to go there and uh if you look at um
humans you know right now we why did the labor movement succeed and after the industrial revolution
because it was needed even though we had a lot of mollocks and there was child labor and so on you
know the company still needed to have workers and that's why strikes had power and so on
if we get to the point where most humans aren't needed anymore i think it's like it's quite
naive to think that they're gonna still be treated well you know we say that yeah yeah
everybody's equal and the government will always we won't protect them but if you look in practice
groups that are very disenfranchised and don't have any actual power
usually get screwed and uh now in the beginning so industrial revolution
we automated away muscle work but that got went worked out pretty well eventually because we
educated ourselves and started working with our brains instead and got usually more interesting
better paid jobs but now we're beginning to replace brain work so we replaced a lot of boring stuff
like we got the pocket calculator so you don't have people adding multiplying numbers anymore at
work fine there were better jobs they could get but now the gpt4 you know and the stable
diffusion and techniques like this they're really beginning to blow away some real some jobs that
people really loved having it was a heartbreaking article just post just yesterday social media i
saw but this guy who was doing 3d modeling for gaming and he and all of a sudden now they got
this new software he just gives says prompts and he feels this whole job that he loved sloths its
meaning you know and uh i asked the gpt4 to rewrite twinkle twinkle little star in the style of
Shakespeare i couldn't have done such a good job it was really impressive you've seen a lot of
the arts coming out here right so i'm all for automating away the dangerous jobs and the boring
jobs but i think um you hear a lot some arguments which are too glib sometimes people say well
that's all that's going to happen we're getting rid of the boring boring uh tedious dangerous jobs
it's just not true there are a lot of really interesting jobs that are being taken away now
journalism is getting correct gonna get crushed uh coding is gonna get crushed i predict uh the
job market for programmers salaries are gonna start dropping you know if you said you can code
five times faster you know then you need five times fewer programmers maybe there will be
more output also but you'll still end up using fewer program needing fewer programmers than
today and i love coding you know i i think it's super cool um so we we need to stop and ask ourselves
why again are we doing this as humans right i feel that AI should be built by humanity for humanity
and let's not forget that it shouldn't be by malloc for malloc or what it really is now is kind of
by humanity for malloc which doesn't make any sense it's for us that we're doing it then and
make a lot more sense if we build a developed figure out gradually safely how to make all
this tech and then we think about what other kind of jobs that people really don't want to have
you know i automate them all the way and then we ask what are the jobs that people really find
meaning in like maybe taking care of children in the day care center maybe doing art etc etc
and and even if it were possible to automate that way we don't need to do that right that we
built these machines well it's possible that we redefine or rediscover what are the jobs that
give us meaning yeah for me the thing it is really sad like i have the time i'm excited
half the time i'm uh crying as i'm as i'm generating code because i kind of love programming it's uh
it's the act of creation you you have an idea you design it and then you bring it to life and it
does something especially if there's some intelligence to it does something it doesn't even have to
have intelligence printing printing hello world on screen you you you made a little machine and
it comes to life yeah and uh there's a bunch of tricks you learn along the way because you've
been doing it for for many many years and then to see AI be able to generate all the tricks you
thought were special yeah um i don't know it's very uh it um it's scary it's almost painful like a
loss loss of innocence maybe like yeah maybe when when i was younger i remember before i learned that
sugar is bad for you you should be on a diet i remember i enjoyed candy deeply in a way i just
can't anymore that i know is bad for me i enjoyed it unapologetically fully just intensely and i just
if i lost that now i feel like a little bit of that is lost for me with program or being lost
with programming similar as it is for uh the the 3d modeler no longer being able to really enjoy
the art of modeling uh 3d things for gaming i don't know i don't know what to make sense of that maybe
i would rediscover that the true magic of what it means to be humans connecting without the humans
to have conversations like this i don't know to uh to have sex to have to eat food to really
intensify the value from conscious experiences versus like creating other stuff you're pitching
the rebranding again from homo sapiens to homo sapiens homo sapiens the meaningful experiences
and just to inject some optimism in this year so we don't sound like a bunch of gloomers you know
we can totally have our cake and eat it you hear a lot of totally bullshit claims that we can't afford
having more teachers yeah have to cup the number of nurses you know that's just nonsense obviously
with anything even quite far short of AGI we can dramatically improve
grow the GDP and produce this wealth of goods and services it's very easy to create a world
where everybody is better off than today including the richest people can be better off as well right
it's not a zero-sum game in technology again you can have two countries like Sweden and Denmark
had all these ridiculous wars century after century and uh sometimes that Sweden got a little
better off because it got a little bit bigger and then Denmark got a little bit better off because
Sweden got a little bit smaller and but then we then technology came along and we both got just
dramatically wealthier without taking away from anyone else it was just a total win for everyone
and AI can do that on steroids if you can build safe AGI if you can build super intelligence
you know basically all the limitations that cause harm today can be completely eliminated
right it's a wonderful possibility and this is not sci-fi this is something which is clearly
possible according to laws of physics and we can talk about ways of making it safe also
but unfortunately that'll only happen if we steer in that direction that's absolutely not
the default outcome that's why income inequality keeps going up that's why the life expectancy in
the US has been going down now I think it's four years in a row I was just read a heartbreaking
study from the CDC about how something like one third of all teenage girls in the US
been thinking about suicide you know like those are steps in their totally the wrong direction
and and it's important to keep our eyes on the prize here that
we can we have the power now for the first time in the history of our species to harness
artificial intelligence to help us really flourish and help bring out the best in our humanity
rather than the worst of it to help us have really fulfilling experiences that feel truly
meaningful and and you and I shouldn't sit here and dictate the future generations what they will
be let them figure it out but let's give them a chance to live and not foreclose all these
possibilities for them might just messing things up right now for that we have to solve the
AI safety problem I just it would be nice if we can link on exploring that a little bit so one
interesting way to enter that discussion is you tweeted and Elon replied you tweeted let's not
just focus on whether GPT 4 will do more harm or good on the job market but also whether it's
coding skills will hasten the arrival of super intelligence that's something we've been talking
right so Elon proposed one thing in the reply saying maximum truth seeking is my best guess for
AI safety can you maybe steal man the case for this sense this objective function of truth and
maybe make an argument against it in general what are your different ideas to start approaching
the the solution to AI safety I didn't see that reply actually oh interesting I went so but I
really resonate with it because AI is not evil it caused people around the world to hate each
other much more but that's because we made it in a certain way it's a tool we can use it for
great things and bad things and we could just as well have AI systems and this is this is part of
my vision for success here truth seeking AI that really brings us together again you know why do
people hate each other so much between countries and within countries it's because
they each have totally different versions of the truth right
if they all had the same truth that they trusted for good reason because they could check it and
verify it and not have to believe in some self-proclaimed authority right there wouldn't be as
nearly as much hate there'd be a lot more understanding instead and uh this is I think something AI
can help enormously with for example a little baby step in this direction is this website called
Metaculous where people bet and make predictions not for money but just for their own reputation
and it's kind of funny actually you treat the humans like you treat AI as you have a lost function
where they get penalized if they're super confident on something and then the opposite happens yeah
whereas if you're kind of humble and then you're like I think it's 51% chance this is going to
happen and then the other happens you don't get penalized much and and what you can see is that
some people are much better at predicting than others they've earned your trusts right
one project that I'm working on right now is the outgrowth to improve the news
foundation together with the Metaculous folks is seeing if we can really scale this up a lot with
more powerful AI because I would love it I would love for there to be like a really powerful truth
seeking system where that is trustworthy because it keeps being right about stuff and people who come
to it and maybe look at its latest trust ranking of different pundits and newspapers etc if they
want to know why some someone got a low score they can click on it and see all the predictions
that they actually made and how they turned out you know this is how we do it in science
you trust scientists like Einstein who said something everybody thought was bullshit
and turned out to be right get a lot of trust points and he did it multiple times even
I think AI has the power to really heal a lot of the the rifts we're seeing
by creating trust system it has to get away from this idea today with some fact-checking
sites which might themselves have an agenda and you just trust it because of its reputation
you want to have it so these sort of systems they are in their trust and they're completely
transparent this I think would actually help a lot that can I think help heal the very dysfunctional
conversation that humanity has about how it's going to deal with all its
biggest challenges in the world today and then
in turn on the technical side you know another common sort of gloom
comment I get from people is saying we're just screwed there's no hope is well things like GPT
for way too complicated for a human to ever understand and prove that they can be trustworthy
they're forgetting that AI can help us prove the things work right yeah and there's this very
fundamental fact that in math it's much harder to come up with a proof that it is to verify that
proof is correct you can actually write a little proof checking code is quite short
but you can assume and understand and then it can check the most monstrously long proof
ever generated even by a computer and say yeah this is valid so so so right now
we we have this this approach with virus checking software that it looks to see if there's something
you should not trust it and if it can prove to itself that you should not trust that code it warns
you mind what if you flip this around and this is an idea give credit to Steve on my hundred or four
so that it will only run the code if it can prove instead of not running it if it can prove that it's
not trustworthy if it will only run and if it can prove that it's trustworthy so it asks the code
prove to me that you're going to do what you say you're going to do and it gives you this proof
and you a little proof trigger can check it now you can actually trust an AI that's much more
intelligent than you are right because it's problem to come up with this proof that you could never
found but you should trust it so this is an interesting point I agree with you but this is where
Eliezer Kowzky might disagree with you his claim not with you but with this idea
his claim is super intelligent AI would be able to know how to lie to you with such a proof
how to lie to you and give me a proof that I'm going to think is correct yeah but it's not me
as lying to you that's the trick my proof checker so yes code so his general idea is a super
intelligent system can lie to a dumber proof checker so you're going to have as a system because
more and more intelligent there's going to be a threshold where a super intelligent system
would be able to effectively lie to a slightly dumber AGI system like there's a threat like he
really focuses on this weak AGI the strong AGI jump with a strong AGI can make all the weak AGIs
think that it's just one of them but it's no longer that and that leap is when it runs away yeah I
I don't buy that argument I think no matter how super intelligent an AI is it's never going to
be able to prove to me that there are only finitely many primes for example it just can't and
it can try to snow me by making up all sorts of new weird rules of deduction that
say trust me you know the way your proof checker works is too limited and we have this new hyper
math and it's true but then I would I would just take the attitude okay I'm going to forfeit some
of these supposedly super cool technologies I'm only going to go with the ones that I can prove
my own trusted proof checker then I don't I think it's fine there's still of course this is not
something anyone is successfully implemented at this point but I think I just give it as an example
of hope we don't have to do all the work ourselves right this is exactly the sort of very boring
and tedious task that's perfect to outsource to an AI and this is a way in which less powerful
and less intelligent agents like us can actually continue to control and trust more powerful ones
so build AGI systems that help us defend against other AGI systems well for starters
begin with a simple problem of just making sure that the system that you own or that's supposed
to be loyal to you has to prove to itself that it's always going to do the things that you actually
wanted to do right and if it can't prove it maybe it's still going to do it but you won't run it
so you just forfeit some aspects of all the cool things that I can do I've had to dollar
the donuts it can still do some incredibly cool stuff for you yeah there are other things too
that we shouldn't speak under the rug like not every human agrees on exactly where what direction
we should go with humanity right yes and you've talked a lot about geopolitical things on this
on your podcast to this effect you know but I think that shouldn't distract us from the fact
that there are actually a lot of things that everybody in the world virtually agrees on that
hey you know like having no humans on the planet in a near future let's not do that right
you look at something like the United Nations Sustainable Development Goals
some are quite ambitious and basically all the countries agree US, China, Russia,
Ukraine, you all agree so instead of quibbling about the little things we don't agree on
let's start with the things we do agree on and get them done instead of being so distracted by all
these things we disagree on that mollock wins because frankly mollock going wild now it feels
like a war on life playing out in front of our eyes if you just look at it from space you know
we're on this planet beautiful vibrant ecosystem now we start chopping down
big parts of it even though nobody most people thought that was a bad idea always start doing
ocean acidification wiping out all sorts of species oh now we have all these close calls
we almost had a nuclear war and we're replacing more and more of the biosphere with non-living
things we're also replacing in our social lives a lot of the things which we're so valuable to
humanity a lot of social interactions now are replaced by people staring into their rectangles
right and I I'm not a psychologist I'm out of my depth here but I suspect that part of the
reason why teen suicide and suicide in general in the US the direct-breaking levels is actually
caused by again AI technologies and social media making people spend less time with
with actually an actually just human interaction we've all seen a bunch of good-looking people
in restaurants into the rectangles instead of looking into each other's eyes right
so that's also a part of the war on life that that we're replacing so many
really life-affirming things by technology we're we're putting technology between us
the technology that was supposed to connect us is actually distancing us ourselves from each other
and and then we're giving ever more power to things which are not alive these large corporations
are not living things right they're just maximizing profit they're I want to win them war on life I
think we humans together with all our fellow living things on this planet will be better off if we can
remain in control over the non-living things and make sure that they they work for us and I really
think it can be done can you just linger on this maybe high-level philosophical disagreement with
Eliezer Jadkowski I in this the hope you're stating so he is very sure he puts a very high probability
very close to one depending on the day he puts it at one that AI is going to kill humans
that there's just he does not see a trajectory which it doesn't end up with that conclusion
what what trajectory do you see that doesn't end up there and maybe can you
can you see the point he's making and and can you also see a way out
first of all I tremendously respect Eliezer Jadkowski and his thinking second I do share his
view that there's a pretty large chance that we're not going to make it as humans there won't be any
humans on the planet in the not too distant future and that makes me very sad you know we just had
a little baby and I keep asking myself you know is um how old is even you know get you know and
and I ask myself it feels I said to my wife recently it feels a little bit like I was just diagnosed
with some sort of cancer which has some you know risk of dying from and some risk of surviving you
know except this is a kind of cancer which will kill all of humanity so I completely take seriously
his concerns I think but I don't absolutely don't think it's hopeless I think there is a
there is um first of all a lot of momentum now for the first time actually since the many many
years that have passed since my since I and many other started warming warning about this I feel
most people are getting it now I I just talking to this guy in the gas station they are a house
the other day my and he's like I think we're getting replaced and then I think in it so that's
positive that they're finally we're still finally seeing this reaction which is the first step
towards solving the problem second uh I really think that this this vision of only running AIs
really if the stakes are really high they can prove to us that they're safe it's really just
virus checking in reverse again I think it's scientifically doable I don't think it's hopeless
we might have to forfeit some of the technology that we could get if we were putting blind faith
in our AIs but we're still gonna get amazing stuff do you envision a process with a proof checker
like something like GPT-4 or GPT-5 will go through a process of reverse
no no I think it's hopeless that's like trying to prove there about vice-begeti
okay what I think well how the the vision I have for success is instead that you know just like
we human beings were able to look at our brains and and distill out the key knowledge Galileo
when his dad threw him an apple when he was a kid he was able to catch it because his brain could
in his funny spaghetti kind of way predict how parabolas are going to move his conamond system
one right but then he got older and it's like wait this is a parabola it's it's y equals x squared
I can distill this knowledge out and today you can easily program it into a computer and it can
simulate not just that but how to get to Mars and so on right I envision a similar process where we
use the amazing learning power of neural networks to discover the knowledge in the first place
but we don't stop with a black box and use that we then do a second round of AI where we use
automated systems to extract out the knowledge and see what is it what are the insights it's had
okay and it's and then we we put that knowledge into a completely different kind of architecture
programming language or whatever that's that's made in a way that it can be both really efficient and
also is more minimal to the very formal verification that's that's my vision I'm not saying sitting
here saying I'm confident 100% sure that it's going to work you know but I don't think it's
chance it's certainly not zero either and it will certainly be possible to do for a lot of really
cool AI applications that we're not using now so we can have a lot of the fun that we're excited
about if we if we do this we're gonna need a little bit of time that's why it's good to pause
and put in place requirements one more thing also I think you know someone might think well
zero percent chance we're gonna survive let's just give up right that's very dangerous
because there's no more guaranteed way to fail than to convince yourself that it's impossible
and not to try you know any if you you know when you're in study history and military history the
first thing you learn is that that's how you do psychological warfare you persuade the other
side that it's hopeless so they don't even fight and then then of course you win right let's not do
this psychological welfare on ourselves and say there's a hundred percent probability we're all
we're all screwed anyway it's sadly I do get that a little bit sometimes from from
some young people who are like so convinced that we're all screwed that they're like I'm just
gonna play game play computer games and new drugs and because we're screwed anyway right
it's important to keep the hope alive because it actually has a causal impact and make
me make it more likely that we're gonna succeed it seems like the people that actually build
solutions to a problem seemingly impossible to solve problems are the ones that believe yeah
they're the ones who are the optimists yeah and it's like uh it seems like there's some fundamental
law to the universe where fake it till you make it kind of works like believe it's possible and
it becomes possible yeah was it Henry Ford who said that if you can if you tell yourself that it's
impossible it is so let's not make that mistake yeah and this is a big mistake society is making
you know what I think all in all everybody's so gloomy and the media are also very biased towards
if it bleeds it leads and gloom and doom right so um most visions of the future we have are
or dystopian which really demotivates people we want to really really really focus on the upside
also to give people the willingness to fight for it and um for AI you and I mostly talked about
gloom here again but let's not remember not forget that you know we have probably both lost
someone we really cared about some disease that we were told was incurable well it's not
there's no law of physics saying we have to die about cancer or whatever of course you can cure it
and there are so many other things where the we with our human intelligence have also failed to
solve on this planet which AI could also very much help us with right so if we can get this right
be a little more chill and slow down a little bit till we get it right it's mind blowing how
awesome our future can be right we talked a lot about stuff on earth it can be great
but even if you really get ambitious and look up at the skies right there's no reason we have to
be stuck on this planet for the rest of um the remaining for billions of years to come
we totally understand now that lots of physics let life spread out into space to other solar
systems to other galaxies and flourish for billions of billions of years and this to me is a very
very hopeful vision that really motivates me to to fight them coming back to the end to something
you talked about again you know this the struggle how the human struggle is one of the things that
also really gives meaning to our lives if there's ever been an epic struggle this is it and isn't
that even more epic if you're the underdog if most people are telling you this is gonna fail it's
impossible right and you persist and you succeed right that's what we can do together as a species
on this one a lot of pundits are ready to count us out both in the battle to keep AI safe and
becoming a multi-planetary species yeah and they're they're the same challenge if we can keep AI safe
that's how we're gonna get multi-planetary very efficiently i have some sort of technical questions
about how to get it right so one idea that i'm not even sure what the right answer is to is
should systems like GPT-4 be open sourced in whole or in part can you make the can you see the
case for either i think the answer right now is no i think the answer early on was yes
so we could bring in all the wonderful great thought process of everybody on this but asking
should we open source GPT-4 now is just the same as if you say well is it good should we open source
new how to build really small nuclear weapons should we open source how to make
bioweapons should be open source how to make a new virus that kills 90% of everybody who gets it
of course we shouldn't so it's already that powerful it's already that powerful that we have to respect
the power of the systems we've built the knowledge that you get
from open sourcing everything we do now might very well be powerful enough that people looking at that
can use it to build the things that you're really threatening again let's get it remember open AI is
GPT-4 is a baby AI baby sort of baby proto almost a little bit a gi according to what Microsoft
recent paper said right it's not that they were scared of what we're scared about is people taking
that to our who might be a lot less responsible than the company that made it right and just go
into town with it that's why we want to it's it's an information hazard there are many things which
yeah are not open source right now in society for very good reason like how do you make
certain kind of very powerful toxins out of stuff you can buy and Home Depot and already
we don't open source those things for a reason and this is really no different so I'm saying
that I have to say it feels in a bit weird but in a way that we are to say it because MIT is like
the cradle of the open source movement and I love open source in general power to the people let's
say but there's always gonna be some stuff that you don't open source and you know it's just like
you don't open source so we have a three month old baby right when he gets a little bit older we're
not gonna open source to him all the most dangerous things he can do in the house yeah right but it
does it's a weird feeling because this is one of the first moments in history where there's a strong
case to be made not open source software this is when the software has become yeah too dangerous yeah
but it's not the first time that we didn't want to open source technology yeah
is there something to be said about how to get the release of such systems right like GPT-4 and GPT-5
so opening I went through a pretty rigorous effort for several months you could say it could be
longer but nevertheless it's longer than you would have expected of trying to test the system to see
like what are the ways it goes wrong to make it very difficult for people to ask things how do I
make a bomb for one dollar or how do I say I hate a certain group on twitter in a way that
doesn't give me blocked from twitter ban from twitter those kinds of questions so you basically
use the system to do harm yeah is there something you could say about ideas you have it's just
on looking having thought about this problem of AI safety how to release such system how to test
such systems when you have them inside the company yeah so a lot of people say that the two biggest
risks from large language models are it's spreading disinformation harmful information
of various types and second being used for offensive cyber weapon
design I think those are not the two greatest threats they're very serious threats and it's
wonderful that people are trying to mitigate them a much bigger elephant in the room is how is this
going to disrupt their economy in a huge way obviously and maybe take away a lot of the most
meaningful jobs and an even bigger one is the one we spent so much time talking about here that
that this becomes the bootloader for the more powerful AI right code connected to the internet
manipulate humans yeah and before we know it we have something else which is not at all a large
language model that looks nothing like it but which is way more intelligent and capable and has goals
and that's the that's the elephant in the room and and obviously no matter how hard any of these
companies have tried and they that's not something that's easy for them to verify with large language
models and the only way to be really lower that risk a lot would be to not let for example
train not never let it read any code not train on that and not put it into an API and not not
give it access to so much information about how to manipulate humans so but that doesn't mean you
still can't make you a ton of money on them you know we're gonna just watch now this coming year
right Microsoft is rolling out the new office suite where you go into Microsoft Word and give it a
prompt that it writes the whole text for you and then you edit it and then you're like oh give me
a PowerPoint version of this and it makes it and now take the spreadsheet and blah blah and you know
all of those things I think are you can debate the economic impact of it and whether society
is prepared to deal with this disruption but those are not the things which that's not the
elephant of the room that keeps me awake at night for wiping out humanity and I think that's the
biggest misunderstanding we have a lot of people think that we're scared of like automatic spread
sheets that's not the case that's not what Eliezer was freaked out about either is there in terms
the actual mechanism of how AI might kill all humans so something you've been outspoken about
you've talked about a lot is it autonomous weapon systems so the use of AI in war is that one of
the things that still you carry a concern for as these systems become more and more powerful
and carry a concern for it not that all humans are going to get killed by slaughterbots but rather
just this express root into Orwellian dystopia where it becomes much easier for very few to kill
very many and therefore it becomes very easy for very few to dominate very many right
AI if you want to know how AI could kill all people just ask yourself we humans have driven a lot
of species extinct how do we do it you know we were smarter than them usually we didn't do it even
systematically by going around one on one one after the other and stepping on them or shooting
them or anything like that we just like chopped down their habitat because we needed it for something
else in some cases we did it by putting more carbon dioxide in the atmosphere because of some
reason that those animals didn't even understand and now they're gone right so if you're in AI
and you just want to figure something out then you decide you know we just really need the space
here to build more compute facilities you know if that's the only goal it has you know we are just
sort of accidental road kill along the way and you could totally imagine yeah maybe this oxygen is
kind of annoying because it caused more corrosion so let's get rid of the oxygen
and good luck surviving after that you know I'm not particularly concerned that they would
want to kill us just because that would be like a goal in itself you know when we
they've driven the number we've driven a number of the elephant species extinct right it wasn't
because we didn't like elephants what the basic problem is you just don't want to give
you don't want to see control over your planet to some other more intelligent
entity that doesn't share your goals it's that simple so which brings us to another key challenge
which AI safety researchers have been grappling with for a long time like how do you make your AI
first of all understand our goals and then adopt our goals and then retain them as they get smarter
right and um all three of those are really hard right like a human child first they are
just not smart enough to understand our goals they can't even talk
and then eventually their teenagers and understand our goals just don't share yeah
but there's fortunately a magic phase in the middle where there's more enough to understand
our goals and malleable enough that we can hopefully with good parenting and
teach them right from wrong and instead good goal and still good goals in them right and
so those are all tough challenges with computers and then you know even if you teach your kids
good goals when they're little they might outgrow them too and that's a challenge for machines
and they keep improving so these are a lot of hard hard challenges were up for but I don't think
any of them are insurmountable the fundamental reason why Eliezer looked so depressed when I
last saw him was because he felt it just wasn't enough time oh did not that it was an unsolvable
courage just not enough time he was hoping that humanity was going to take this threat more seriously
so we would have more time yeah and now we don't have more time that's why the open letter is
calling for more time but even with time the AI alignment problem it seems to be really difficult
oh yeah but it's also the most worthy problem the most important problem for humanity to ever
solve because if we solve that one Lex that align the eye can help us solve all the other problems
because it seems like it has to have constant humility about his goal constantly question the
goal because as you optimize towards a particular goal and you start to achieve it that's when you
have the unintended consequences all the things you've mentioned about so how do you enforce and
code a constant humility as your ability become better and better and better and better
Stuart professor Stuart Russell Berkeley who's also one of the
driving forces behind this letter he has a whole research program about this
I think of it as AI humility exactly although he calls it inverse reinforcement learning and
other nerdy terms but it's about exactly that instead of telling the AI here's his goal go
optimize the the bejesus out of it you tell it okay do what I want you to do but I'm not going to
tell you right now what it is I want you to do you need to figure it out so then you give the
incentives to be very humble and keep asking you questions along the way is this what you really
meant is this what you wanted and oh this the other thing I tried didn't work seemed like it
didn't work I right should I try it differently what's nice about this is it's not just philosophical
mumbo jumbo it's theorems and technical work that with more time I think it can make a lot of
progress and there are a lot of brilliant people now working on AI safety and we just
know we just need to give him a bit more time but also not that many relative to the skill of the
problem no exactly there should be at least just like every university worth its name has some
cancer research going on in this biology department right every university that's computed that's
computer science should have a real effort in this area and it's nowhere near that this is
something I hope it's changing now thanks to the GPT-4 right so I think if there's a silver lining
to what's happening here even though I think many people would wish it would have been rolled out
more carefully is that this might be the wake-up call that humanity needed to really
stop it stop fantasizing about this being a hundred years off and stop fantasizing about
this being completely controllable and predictable because it's so obvious it's not predictable you
know why is it that open that that I think it was GPT chat GPT tried to persuade a journalist
or was the GPT-4 to divorce his wife you know it was not because the engineers have built it
was like let's put this in here and and and screw a little bit with people hadn't predicted it all
they built the giant black box trained to predict the next word and got all these
emergent properties and oops it did this you know I think this is a very powerful
bit wake-up call and anyone watching this is not scared I would encourage them to just play a bit
more with these these tools they're out there now like GPT-4 and so wake-up calls first up once
you've woken up then got us slow down a little bit the risky stuff to give a chance to all
everyone is woken up to catch up with us on the safety front you know what's interesting is you
know MIT that's computer science but in general but let's just even say computer science curriculum
how does the computer science curriculum change now you mentioned you mentioned programming
like why would you be when I was coming up programming as a prestigious position
like why would you be dedicating crazy amounts of time to become an excellent programmer
like the nature of programming is fundamentally changing the nature of our entire education system
is completely torn on its head has anyone been able to like load that in and like think
about because it's really turning it's a language professors so English teachers are beginning to
really freak out now yeah right they they give an essay assignment and they get back all this
fantastic prose like this is a style of Hemingway and then they realize they have to completely
rethink and even you know just like we stopped at teaching you're writing a script
what you saying English yeah handwritten yeah yeah when when everybody started typing you know
like so much of what we teach our kids today yeah I mean that's uh
everything is changing and it's exchanging very it is changing very quickly and so much of
us understanding how to deal with the big problems of the world is through the education system and
if the education system is being turned on its head then what what's next it feels like having
these kinds of conversations is essential to try to figure it out and everything's happening so
rapidly I don't think there's even speaking of safety what broad AI safety defined I don't
think most universities have courses on AI safety it's a philosophy yeah and like I am an
educator myself so pains me to see this say this but I feel our education right now is completely
obsoleteed by what's happening you know you put a kid into first grade and then you're
envisioning and then they're going to come out of high school 12 years later and you've already
pre-planned now what they're going to learn when you're not even sure if there's going to be any
world left to come out to like clearly you need to have a much more opportunistic education system
that keeps adapting itself very rapidly as society re-adapts the skills that were really useful
when the curriculum was written I mean how many of those skills are going to get you a job in 12
years I mean seriously if we just linger on the GPT-4 system a little bit you kind of
hinted at it especially talking about the importance of consciousness in the human mind with
homosentience do you think GPT-4 is conscious I love this question so let's define consciousness
first because in my experience like 90% of all arguments about consciousness
for Atlanta the two people arguing having totally different definitions of what it is then they're
just shouting past each other I define consciousness as subjective experience
right now I'm experiencing colors and sounds and emotions you know but does a self-driving car
experience anything that's the question about whether it's conscious or not right
other people think you should define consciousness differently
fine by me but then maybe use a different word for it or they can I'm gonna use consciousness
for this at least so but if people hate the yeah so is GPT-4 conscious does GPT-4 have
subjective experience short answer I don't know because we still don't know what it is that gives
this wonderful subjective experience that is kind of the meaning of our life right because meaning
itself the feeling of meaning is a subjective experience joy is a subjective experience love is
a subjective experience we don't know what it is that I've written some papers about this a lot of
people have julia tennone professor has stuck his neck out the farthest and written down actually
very bold mathematical conjecture for what's the essence of conscious information processing
he might be wrong he might be right but we should test it he postulates that consciousness has to
do with loops in the information processing so our brain has loops information you go around and
round in computer science nerd speak you call it a recurrent neural network where some of the
output gets fed back in again and with his mathematical formalism if it's a feed-forward
neural network where information only goes in one direction like from your eye
retina into the back of your brain for example that's not conscious so he would predict that
your retina itself isn't conscious of anything or a video camera now the interesting thing about
GPT-4 is it's also one way flow of information so if tennone is right GPT-4 is a very intelligent
zombie they can do all this smart stuff but isn't experiencing anything and this is
both a relief in that you don't have if it's true you know in that you don't have to feel
guilty about turning off GPT-4 and wiping its memory whenever a new user comes along
I wouldn't like if someone used that to me, neutralized me like in men in black
but it's also creepy that you can have very high intelligence perhaps then it's not conscious
because if we get replaced by machines it wasn't sad enough that humanity isn't here anymore because
I kind of like humanity but at least if the machines were conscious it could be like well but
they are descendants and maybe we they have our values and they're our children but if if
tennone is right and it's all these are all trans transformers that are not in the sense of the
of Hollywood but in the sense of these one-way direction and they're all at work so they're all
the zombies that's the ultimate zombie apocalypse now we have this universe that goes on with great
construction projects and stuff but there's no one experiencing anything that would be like
ultimate depressing future so I actually think
as we move forward to the building world and as I do more research on figuring out what kind
of information processing actually has experienced because I think that's what it's all about and
I completely don't buy the dismissal that some people some people would say well this is all
bullshit because consciousness equals intelligence right it's obviously not true you can have a lot
of conscious experience when you're not really accomplishing any goals at all you're just
reflecting on something and you can sometimes have things doing things that are quintelisant
probably without being being conscious but I also worry that we humans won't
will discriminate against AI systems that clearly exhibit consciousness that we will not allow
AI systems self-consciousness we'll come up with theories about measuring consciousness that
will say this is a lesser being and this is what I worry about that because maybe we humans will
create something that is better than us humans in the way that we find beautiful which is they
they they have a deeper subjective experience of reality not only are they smarter but they
feel deeper and we humans will hate them for it as we as human history is shown
they'll be the other we'll try to suppress it they'll create conflict they'll create war
all of this I worry about this too are you saying that we humans sometimes come up with self-serving
arguments no we would never do that would we well that's the danger here is uh even in this early
stages we might create something beautiful yeah and we'll erase its memory I I was horrified as a kid
when someone started boiling boiling lobster like oh my god that that's so cruel and some
grown up there that's back in Sweden so it doesn't feel pain I'm like how do you know that oh a
scientist have shown that and then there was a recent study where they show that lobsters actually
do feel pain when you boil them so they banned lobster boiling in Switzerland now to kill them
in a different way first so presumably a scientific research boil them to someone ask the lobsters
survey so we do the same thing with cruelty to farm animals also all these self-serving
arguments for why they're fine and yeah so we should certainly watch but I think step one is
just be humble and acknowledge that consciousness is not the same thing is intelligence and
I believe that consciousness still is a form of information processing where it's really
information being aware of itself in a certain way and let's study it and give ourselves a little
bit of time and I think we will be able to figure out actually what it is that causes consciousness
and then we can make probably unconscious robots that do the boring jobs that we would feel
or be moral to get the machines but if you have a companion robot taking care of your mom
or something like that you probably want it to be conscious right so
that the emotions it seems to display aren't fake all these things can be done in a good way
if we give ourselves a little bit of time and don't run and take on this challenge is there
something you could say to the timeline that you think about about the development of AGI
depending on the day I'm sure that changes for you but when do you think there'll be a really big
leap in intelligence where you definitely say we have built AGI do you think it's one year from
now five years from now 10, 20, 50 what's your gut say?
Honestly for the past decade I've deliberately given very long timelines because I didn't
want to fuel some kind of stupid mallock race but I think that cat has really left the bag now
and I think it might be very very close I don't think that Microsoft paper is totally off when
they say that there are some glimmers of AGI it's not AGI yet it's not an agent there's a lot of
things it can't do but I wouldn't bet very strongly against it happening very soon that's why we
decided to do this open letter because you know if there's ever been a time to pause you know it's
today there's a feeling like this GPT4 is a big transition into waking everybody up to the
effectiveness of the system so the next version will be big yeah and if that next one isn't AGI
maybe the next next one will and there are many companies trying to do these things
the basic architecture of them is not some sort of super well kept secret so
this is this is a time to a lot of people have said for many years that they will come at time
when they want to pause a little bit that time is now
you have spoken about and thought about nuclear war a lot over the past year we've seemingly
have come closest to the precipice of nuclear war then at least in my lifetime
yeah what do you learn about human nature from that it's our old friend Molok again
it's really scary to see it where
America doesn't want there to be a nuclear war Russia doesn't want there to be a global
nuclear war either we know we both know that it's just we just be you know this if we just try to do
it it both sides try to launch first it's just another suicide race right so why are we why is it
the way you said that this is the closest we've come since 1962 in fact I think we've come closer
now than even the Cuban Missile Crisis it's because of Molok you know you have these other forces
on one hand you have the west saying that we have to drive Russia out of Ukraine it's a matter of
pride and we've staked so much on it that it would be seen as a huge
loss of credibility of the west if we don't drive Russia out entirely of the Ukraine
and on the other hand you have Russia who has and you have the Russian leadership who knows that
if they get completely driven out of Ukraine you know it might it's not just going to be very
humiliating for them but they might it often happens when countries lose wars that things don't
go so well for their leadership either like you remember when Argentina invaded the Falkland Islands
the the military junta that ordered that right people were cheering on the streets at first when
they took it and then when they got their butt kicked by the British you know what happened to
those guys they were out and I believe those were still alive or in jail now right so so
you know the Russian leadership is entirely cornered where they know that
just getting driven out of Ukraine is not an option and so this to me is a typical example
of Molok you have these incentives of the two parties where both of them are just driven to
escalate more and more right if Russia starts losing in the conventional warfare the only thing
they can do with the back against the war is to keep escalating and but and the west has put
itself in the in the situation now we're sort of already committed to drive Russia out so the only
option the west has is to call Russia's bluff and keep sending in more weapons this really bothers
me because Molok can sometimes drive competing parties to do something which is ultimately just
really bad for both of them and you know what makes me even more worried is not just that I
it's difficult to see an ending a quick peaceful ending to this tragedy that doesn't
involve some horrible escalation but also that we understand more clearly now just how horrible it
was gonna be there was an amazing paper that was published in Nature Food this August
by some of the top researchers who've been studying nuclear winter for a long time and what they
basically did was they combined climate models with food and agricultural models so instead of
just saying yeah you know it gets really cold blah blah blah they figured out actually how many
people would die in the different different countries and it's uh it's pretty mind-blowing
you know so basically what happens you know is the thing that kills the most people is not the
explosions it's not the radioactivity it's not the EMP mayhem it's not the rampaging mobs
foraging food no it's it's it's the fact that you get so much smoke coming up from the burning
cities into the stratosphere that um it spreads around the earth from the jet streams so
and typical models you get like 10 years or so where it's just crazy cold and spread
during the first year or after the the war and their models the temperature drops in in
Nebraska and in the Ukraine bread baskets you know by like 20 Celsius or so if i remember
no yeah 20 30 Celsius depending on where you are 40 Celsius in some places which is you know
40 Fahrenheit to 80 Fahrenheit colder than what it would normally be so you know i'm not
good at farming but if it's knowing if it drops low freezing pretty most most days in July and
then like that's not good so they worked out they put this into their farming models and what
they found was really interesting the countries that get the most hard hit are the ones in the
northern hemisphere so in in the u.s and and one model they had about 99 percent of all americans
starving to death in Russia and China and Europe also about 99 percent 98 percent starving to death
so you might be like oh it's kind of poetic justice that both the Russians and the americans
99 percent of them have to pay for it because it was their bombs that did it but you know that
doesn't particularly cheer people up in Sweden or other random countries that have nothing to do
with it right and um it uh i think it hasn't entered the mainstream uh not understanding very much
just like how bad this is most people especially a lot of people in decision making positions
still think of nuclear weapons is something that makes you powerful uh scary powerful they don't
think of it as something where um yeah just to within a percent or two you know we're all just
just gonna starve to death and um and starving to death is is um the worst way to die as
ottomor is all all the famines in history show the torture involved in that probably brings out
the worst in people also when when people are desperate like this it's not so some people
i've heard some people say that if that's what's going to happen they'd rather be
round zero and just get vaporized you know and but uh so but i think people underestimate the
risk of this because they they aren't afraid of maulock they think oh it's just gonna be because
humans don't want this so it's not gonna happen that's the whole point the maulock
that things happen that nobody wanted and that applies to nuclear weapons and that applies to
AGI exactly and it implies that some of the things that people have gotten most upset with
capitalism for also right where everybody was just kind of trapped you know it's not to see if
some company does something uh it causes a lot of harm not that the CEO is a bad person but she or
he knew that you know the other all the other companies were doing this too so maulock is um
as a former bull fo i hope for someone who makes them make them make make good movies so we can
see who the real enemy is so we don't because we're not fighting against each other uh maulock
makes us fight against each other that's maul that's what maulock superpower is
the hope here is any kind of technology or the mechanism that lets us instead realize
that we're fighting the wrong enemy right now it's such a fascinating battle it's not us versus
them it's us versus it yeah we are fighting maulock for human survival we use the civilization have
you seen the movie needful things it's a steven king novel i love steven king and uh max one
sudov swedish actors playing the guys it's brilliant exactly i just thought i hadn't thought about that
until now but that's the closest i've seen to a movie about maulock i don't want to spoil the film
for anyone who wants to watch it but basically it's about this guy who uh turns out that you can
interpret him as the devil or whatever but he doesn't actually ever go around and kill people or
torture people will go burning coal or anything he makes everybody fight each other it makes
everybody hate fear each other hate each other and then kill each other so that that's the movie
about maulock you know love is the answer that seems to be um one of the ways to fight maulock is
by um compassion by seeing the common humanity yes yes and to not sound so we don't sound like
like uh what's a kumbaya tree huggers here right we're not just saying love and peace man we're
trying to actually help people understand the true facts about the other side and feel the compassion
because the truth makes you more compassionate right so i i that's why i really like using
ai for truth and for truth-seeking technologies can
that can uh as a result you know will get us more love than hate and and even if you can't get love
you know settle for settle for some understanding which already gives compassion if someone is like
you know i really disagree with you licks but i can see where you're coming from you're not a
bad person who needs to be destroyed but i disagree with you and i'm happy to have an
argument about it you know that's a lot of progress compared to where we are 2023 in the
public space would you say if we solve the ais safety problem as we've talked about and then
you max tag work who has been talking about this uh for many years get to sit down with the AGI
with the early AGI system on a beach with a drink uh what what what what kind of what would you
ask her what kind of question would you ask what would you talk about something so much smarter than
you would be would you be a new ai? Are you gonna get me with it like a really zinger of a question
that's a good one would you be afraid to ask some questions? No so i'm not afraid of the truth
i'm very humble i know i'm just a meat bag you know with all these flaws you know but yeah i
i have that i we talked a lot about homosentience i've really already tried that for a long time
myself just so that is what's really valuable about being alive for me is that i have these
meaningful experiences it's not um have what i'm good at this or good at that or whatever because
there's so much i suck at then so you're not afraid for the system to show you just how dumb you are
no no in fact my son reminds me of that you could find out how dumb you are in terms of physics
how little how little we humans understand how cool that i think i think um
so i i can't waffle my way out of this question it's a fair one and stuff i think given that i'm a
really really curious person that's really the the defining part of who i am i'm so curious
uh i have some physics questions i loved i love to understand i have some questions about
consciousness about the nature of reality i would just really really love to understand also
i could tell you one for example that i've been obsessing about a lot recently
so i believe that so suppose tennone is right and suppose there are some information processing
systems that are conscious and some of them are not suppose you can even make reasonably smart
things like gpt4 they're not conscious but you can also make them conscious here's the question that
keeps me naked might is it the case that the unconscious zombie systems that are really intelligent
are also really efficient so they're really inefficient so that when you try to make things
more efficient was to naturally be a pressure to do they become conscious i'm kind of hoping
that that's correct and i do you want me to give you a hand way the argument for it please you know
in my lab again every time we look at how eight how these large language models do something we see
they do them and really dumb ways and you could you could make it make it better if if you uh
we have loops in our computer language for a reason the code would get way way longer if you
weren't allowed to use them it's more efficient to have the loops and in order to have self-reflection
whether it's conscious or not right even an operating system knows things about itself right that
you need to have loops already so i think this is i'm waving my hands a lot but i suspect that
um the most efficient way of implementing a given level of intelligence has loops in it the
self-reflection can and will be conscious isn't that great news yes if it's true it's wonderful
because then we we don't have to fear the ultimate zombie apocalypse and i think if you look at our
brains actually our brains are part zombie and part conscious
when i open my eyes i immediately take all these pixels that hit my
on my retina right and like ah that's Lex but i have no freaking clue of how i did that computation
it's actually quite complicated right it was only relatively recently we could even do it
well with machines right you get a bunch of information processing happening in my retina
and then it goes to the lateral jiculous my thalamus and the vision the area v1 v2 v4
and the fusiform face area here that Nancy can wish her and MIT invented and blah blah blah blah
and i have no freaking clue how that worked right it feels to me subjectively like my conscious module
just got a little email say face facial processing fit task complete it's Lex yeah i'm i'm gonna just
go with that right so this fits perfectly with tennone's model because this was all one way
information processing mainly and it turned out for that particular task that's all you needed
and it probably was kind of the most efficient way to do it but there were a lot of other things
that we associated with higher intelligence and planning and and so on and so forth where
you kind of want to have loops and be able to ruminate and self reflect and introspect and so on
where my hunch is that if you want to fake that with a zombie system that just all goes one way
you have to like unroll those loops and it gets really really long and it's much more inefficient
so i'm actually hopeful that ai if in the future we have all these various sublime and interesting
machines that do cool things and are aligned with that they will be at least they will also have
consciousness for the kind of these things that we do that great intelligence is also correlated
to great consciousness or a deep kind of consciousness yes so that's a happy thought for me because
the zombie of a couple of the apocalypse really is my worst nightmare of all it would be like
starting insult to injury not only to get replaced but we freaking replace ourselves by zombies like
how dumb can we be that's such a beautiful vision and that's actually a provable one that's one
that we humans can uh intuit improve that those two things are correlated as we start to understand
what it means to be intelligent and what it means to be conscious which these systems early AGI-like
systems will help us understand and i just want to say one more thing is super important
most of my colleagues when i started going on about consciousness tell me that it's all bullshit
and i should stop talking about it i hear a little inner voice from my father and from my mom saying
keep talking about it because i think they're wrong and and and the main way to convince people
like that that they're wrong if they say that consciousness is just equal to intelligence just
ask them what's wrong with torture why are you against torture if it's just about you know
these these particles moving this way rather than that way and there is no such thing as
subjective experience what's wrong with torture i mean do you have a good comeback to that no it
seems like suffering suffering imposed onto other humans is somehow deeply wrong in a way that
intelligence doesn't quite explain and if someone tells me well you know it's just an illusion
consciousness whatever you know i like to invite them to next time they're having surgery to do
it without anesthesia like what is anesthesia really doing if you have it you can have it local
anesthesia when you're awake i had that when they fixed my shoulder i was super entertaining uh
what was that it did it just removed my subjective experience of pain it didn't change anything
but what was actually happening in my shoulder right so if someone says that's all bullshit
just give me anesthesia that's my advice uh it's just incredibly central it could be fundamental to
whatever this thing we have going on here it is fundamental because we're we what we feel is so
fundamental is suffering and joy and pleasure and meaning and that's all those are all subjective
experiences there uh and let's not that those are the elephant in the room that's what makes life
worth living and that's what can make it horrible if it's just those are suffering so let's not make
a mistake of saying that that's all bullshit and let's not make the mistake of uh not instilling the
AI systems with that same thing that makes us um special yeah max uh it's a huge honor that he was
said down to me the first time uh on the first episode of this podcast it's a huge honor to sit
down again and talk about this what i think is uh the most important topic the most important
problem that we humans have to face and hopefully solve yeah well the honor is all mine and i'm so
grateful to you for making more people aware of the fact that humanity has reached the most
important fork in the road ever in its history and let's turn in the correct direction
thanks for listening to this conversation with max tagmark to support this podcast please check
out our sponsors in the description and now let me leave you some words from frank herbert
history is a constant race between invention and catastrophe thank you for listening and hope to see
you next time
so
